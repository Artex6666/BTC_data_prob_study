{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bdd5bf42",
      "metadata": {},
      "source": [
        "# Strat√©gie probabiliste sur les contrats up/down Polymarket\n",
        "\n",
        "Ce notebook explore comment estimer en continu la probabilit√© qu'une bougie Bitcoin se cl√¥ture au-dessus ou au-dessous de son prix d'ouverture pour trois horizons (m15, h1, daily en timezone ET), puis comment exploiter les d√©s√©quilibres de cotes observ√©s lors de phases de FOMO.\n",
        "\n",
        "Le pipeline couvre : (1) l'ingestion des donn√©es OHLC minute, (2) l'ing√©nierie de features multi-√©chelles, (3) l'entra√Ænement de mod√®les de probabilit√©s, (4) la simulation d'une cote ¬´ FOMO ¬ª param√©trable et (5) un backtest value simple pour quantifier l'edge potentiel.\n",
        "\n",
        "**üìã Ordre logique d'ex√©cution des sections :**\n",
        "0. Setup & Configuration (imports, constantes, fonctions)\n",
        "1. Chargement et pr√©paration des donn√©es (OHLC + indicateurs)\n",
        "2. Reconstruction multi-√©chelle (snapshots par timeframe)\n",
        "3. Mod√®les intrabougie (entra√Ænement, probabilit√©s, visualisations)\n",
        "4. Probabilit√©s pr√©-ouverture (mod√®les pr√©-open, seuils, visualisations)\n",
        "5. Simulation des cotes FOMO (g√©n√©ration de sc√©narios)\n",
        "   5.1 Calibration du mod√®le FOMO (comparaison avec vraies cotes march√©)\n",
        "6. Backtest ONLINE (trading minute-par-minute, m√©triques)\n",
        "7. Visualisations et analyse (courbes d'√©quity, distributions, timing)\n",
        "8. Backtest ONLINE avec cotes march√© r√©elles\n",
        "9. Lecture mod√®le (feature importances)\n",
        "10. Synth√®se et prochaines √©tapes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228cdd2b",
      "metadata": {},
      "source": [
        "## 0. Setup & Configuration\n",
        "\n",
        "Configuration de l'environnement, imports, constantes et d√©finition de toutes les fonctions utilitaires utilis√©es dans le notebook.\n",
        "\n",
        "- But: centraliser toute la configuration et les fonctions de base.\n",
        "- Entr√©es: aucune.\n",
        "- Sorties: fonctions et constantes globales disponibles pour les sections suivantes.\n",
        "- Lecture: section √† ex√©cuter en premier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9ef0fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "import pathlib\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Iterable, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    brier_score_loss,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    r2_score,\n",
        "    roc_auc_score,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b1b34b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.options.display.max_columns = 120\n",
        "pd.options.display.max_rows = 200\n",
        "\n",
        "DATA_PATH = pathlib.Path(\"../data/btc_1m_OHLC.csv\").resolve()\n",
        "MARKET_ODDS_PATH = pathlib.Path(\"../data/BTC.csv\").resolve()\n",
        "TARGET_TZ = \"America/New_York\"\n",
        "RANDOM_SEED = 17\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbabd36c",
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "plt.rcParams[\"axes.titlesize\"] = 12\n",
        "plt.rcParams[\"axes.labelsize\"] = 11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dbae910",
      "metadata": {},
      "outputs": [],
      "source": [
        "def announce(msg: str) -> None:\n",
        "    print(f\"[INFO] {msg}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f117eca",
      "metadata": {},
      "source": [
        "### 0.1 Fonctions de chargement et indicateurs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fe7c83",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_minute_data(path: pathlib.Path) -> pd.DataFrame:\n",
        "    \"\"\"Charge les donn√©es OHLCV minute et impose un index temporel UTC.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.sort_values(\"timestamp\")\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\", utc=True)\n",
        "    df = df.set_index(\"timestamp\")\n",
        "    df = df.rename(\n",
        "        columns={\n",
        "            \"open\": \"open\",\n",
        "            \"high\": \"high\",\n",
        "            \"low\": \"low\",\n",
        "            \"close\": \"close\",\n",
        "            \"volume\": \"volume\",\n",
        "        }\n",
        "    )\n",
        "    df.index.name = \"timestamp_utc\"\n",
        "    return df\n",
        "\n",
        "\n",
        "def compute_rsi(close: pd.Series, period: int = 14) -> pd.Series:\n",
        "    \"\"\"Calcule un RSI classique sur une s√©rie de cl√¥tures.\"\"\"\n",
        "    delta = close.diff()\n",
        "    gain = delta.clip(lower=0).ewm(alpha=1 / period, adjust=False).mean()\n",
        "    loss = -delta.clip(upper=0).ewm(alpha=1 / period, adjust=False).mean()\n",
        "    rs = gain / (loss + 1e-9)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "\n",
        "def add_global_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    enriched = df.copy()\n",
        "    enriched[\"return_1m\"] = enriched[\"close\"].pct_change().fillna(0.0)\n",
        "    enriched[\"ema_12\"] = enriched[\"close\"].ewm(span=12, adjust=False).mean()\n",
        "    enriched[\"ema_48\"] = enriched[\"close\"].ewm(span=48, adjust=False).mean()\n",
        "    enriched[\"ema_288\"] = enriched[\"close\"].ewm(span=288, adjust=False).mean()\n",
        "\n",
        "    enriched[\"range_1m\"] = (enriched[\"high\"] - enriched[\"low\"]).abs()\n",
        "    enriched[\"atr_15m\"] = enriched[\"range_1m\"].rolling(15).mean().bfill()\n",
        "    enriched[\"sigma_15m\"] = enriched[\"return_1m\"].rolling(15).std().bfill() * math.sqrt(15)\n",
        "\n",
        "    enriched[\"rsi_14\"] = compute_rsi(enriched[\"close\"])\n",
        "    enriched[\"rolling_vol_30\"] = enriched[\"return_1m\"].rolling(30).std().fillna(0.0) * math.sqrt(30)\n",
        "    enriched[\"volume_per_minute\"] = enriched[\"volume\"].rolling(30).mean().bfill()\n",
        "    enriched[\"volume_z\"] = ((enriched[\"volume\"] - enriched[\"volume\"].rolling(120).mean())\n",
        "                            / (enriched[\"volume\"].rolling(120).std() + 1e-9)).fillna(0.0)\n",
        "    enriched[\"trend_ema_ratio\"] = (enriched[\"ema_12\"] - enriched[\"ema_48\"]) / (enriched[\"ema_48\"] + 1e-9)\n",
        "    enriched[\"macro_trend_ratio\"] = (enriched[\"ema_48\"] - enriched[\"ema_288\"]) / (enriched[\"ema_288\"] + 1e-9)\n",
        "    enriched[\"is_trend_up\"] = (enriched[\"trend_ema_ratio\"] > 0).astype(int)\n",
        "    return enriched\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29f554c5",
      "metadata": {},
      "source": [
        "### 0.2 Fonctions de reconstruction multi-√©chelle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d52d31e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_timeframe_snapshots(\n",
        "    df: pd.DataFrame,\n",
        "    freq: str,\n",
        "    label: str,\n",
        "    target_tz: str | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Agr√®ge les minutes en se basant sur les bornes UTC pour √©viter les ambigu√Øt√©s DST.\"\"\"\n",
        "    tz = target_tz\n",
        "    if tz is None:\n",
        "        try:\n",
        "            tz = TARGET_TZ\n",
        "        except NameError:\n",
        "            tz = \"America/New_York\"\n",
        "\n",
        "    local = df.copy()\n",
        "    local[\"bucket_start_utc\"] = local.index.floor(freq)\n",
        "    local[\"bucket_end_utc\"] = local[\"bucket_start_utc\"] + pd.to_timedelta(freq)\n",
        "    local[\"bucket_key\"] = local[\"bucket_start_utc\"]\n",
        "\n",
        "    local[\"timestamp_et\"] = local.index.tz_convert(tz)\n",
        "    local[\"bucket_start_et\"] = local[\"bucket_start_utc\"].tz_convert(tz)\n",
        "    local[\"bucket_end_et\"] = local[\"bucket_end_utc\"].tz_convert(tz)\n",
        "\n",
        "    group = local.groupby(\"bucket_key\", group_keys=False)\n",
        "    local[\"tf_open\"] = group[\"open\"].transform(\"first\")\n",
        "    local[\"tf_high_to_now\"] = group[\"high\"].cummax()\n",
        "    local[\"tf_low_to_now\"] = group[\"low\"].cummin()\n",
        "    local[\"tf_close_to_now\"] = local[\"close\"]\n",
        "    local[\"tf_volume_to_now\"] = group[\"volume\"].cumsum()\n",
        "    local[\"tf_final_close\"] = group[\"close\"].transform(\"last\")\n",
        "    local[\"tf_final_high\"] = group[\"high\"].transform(\"max\")\n",
        "    local[\"tf_final_low\"] = group[\"low\"].transform(\"min\")\n",
        "\n",
        "    local[\"minutes_elapsed\"] = group.cumcount() + 1\n",
        "    local[\"bucket_size\"] = group[\"close\"].transform(\"size\")\n",
        "    local[\"minutes_total\"] = local[\"bucket_size\"].clip(lower=1)\n",
        "    local[\"minutes_remaining\"] = (\n",
        "        local[\"minutes_total\"] - local[\"minutes_elapsed\"]\n",
        "    ).clip(lower=0)\n",
        "    local[\"seconds_remaining\"] = local[\"minutes_remaining\"] * 60\n",
        "    local[\"time_elapsed_ratio\"] = local[\"minutes_elapsed\"] / local[\"minutes_total\"]\n",
        "    local[\"time_remaining_ratio\"] = (\n",
        "        local[\"minutes_remaining\"] / local[\"minutes_total\"]\n",
        "    )\n",
        "\n",
        "    local[\"target_up\"] = (local[\"tf_final_close\"] >= local[\"tf_open\"]).astype(int)\n",
        "\n",
        "    local[\"dist_from_open_pct\"] = (\n",
        "        (local[\"tf_close_to_now\"] - local[\"tf_open\"]) / (local[\"tf_open\"] + 1e-9)\n",
        "    )\n",
        "    local[\"high_gap_pct\"] = (\n",
        "        (local[\"tf_high_to_now\"] - local[\"tf_close_to_now\"]) / (local[\"tf_open\"] + 1e-9)\n",
        "    )\n",
        "    local[\"low_gap_pct\"] = (\n",
        "        (local[\"tf_close_to_now\"] - local[\"tf_low_to_now\"]) / (local[\"tf_open\"] + 1e-9)\n",
        "    )\n",
        "    local[\"running_range_pct\"] = (\n",
        "        (local[\"tf_high_to_now\"] - local[\"tf_low_to_now\"]) / (local[\"tf_open\"] + 1e-9)\n",
        "    )\n",
        "    local[\"minute_body_pct\"] = (\n",
        "        (local[\"close\"] - local[\"open\"]) / (local[\"tf_open\"] + 1e-9)\n",
        "    )\n",
        "\n",
        "    # Distance normalis√©e par ATR 15m (vol en unit√©s de prix)\n",
        "    local[\"z_dist_atr15\"] = (\n",
        "        (local[\"tf_close_to_now\"] - local[\"tf_open\"]) / (local[\"atr_15m\"] + 1e-6)\n",
        "    )\n",
        "    local[\"z_range_atr15\"] = (\n",
        "        (local[\"tf_high_to_now\"] - local[\"tf_low_to_now\"]) / (local[\"atr_15m\"] + 1e-6)\n",
        "    )\n",
        "\n",
        "    local[\"minute_of_day\"] = (\n",
        "        local[\"timestamp_et\"].dt.hour * 60 + local[\"timestamp_et\"].dt.minute\n",
        "    )\n",
        "    local[\"minute_of_week\"] = (\n",
        "        local[\"timestamp_et\"].dt.dayofweek * 1440 + local[\"minute_of_day\"]\n",
        "    )\n",
        "    local[\"minute_of_day_sin\"] = np.sin(2 * np.pi * local[\"minute_of_day\"] / 1440)\n",
        "    local[\"minute_of_day_cos\"] = np.cos(2 * np.pi * local[\"minute_of_day\"] / 1440)\n",
        "    local[\"day_of_week\"] = local[\"timestamp_et\"].dt.dayofweek\n",
        "    local[\"day_of_week_sin\"] = np.sin(2 * np.pi * local[\"day_of_week\"] / 7)\n",
        "    local[\"day_of_week_cos\"] = np.cos(2 * np.pi * local[\"day_of_week\"] / 7)\n",
        "\n",
        "    # Streaks intrabougie (cons√©cutifs)\n",
        "    def _streak_bool(series: pd.Series) -> pd.Series:\n",
        "        count = 0\n",
        "        out = []\n",
        "        for v in series.astype(bool):\n",
        "            if v:\n",
        "                count += 1\n",
        "            else:\n",
        "                count = 0\n",
        "            out.append(count)\n",
        "        return pd.Series(out, index=series.index)\n",
        "\n",
        "    local[\"minute_up\"] = (local[\"close\"] >= local[\"open\"]).astype(int)\n",
        "    local[\"minute_down\"] = 1 - local[\"minute_up\"]\n",
        "    local[\"tf_up_to_now\"] = (local[\"tf_close_to_now\"] >= local[\"tf_open\"]).astype(int)\n",
        "    local[\"tf_down_to_now\"] = 1 - local[\"tf_up_to_now\"]\n",
        "\n",
        "    local[\"streak_up_minute\"] = local.groupby(\"bucket_key\")[\"minute_up\"].apply(_streak_bool)\n",
        "    local[\"streak_down_minute\"] = local.groupby(\"bucket_key\")[\"minute_down\"].apply(_streak_bool)\n",
        "    local[\"streak_tf_up\"] = local.groupby(\"bucket_key\")[\"tf_up_to_now\"].apply(_streak_bool)\n",
        "    local[\"streak_tf_down\"] = local.groupby(\"bucket_key\")[\"tf_down_to_now\"].apply(_streak_bool)\n",
        "\n",
        "    bucket_summary = (\n",
        "        local.groupby(\"bucket_key\")\n",
        "        .agg(\n",
        "            bucket_open=(\"tf_open\", \"first\"),\n",
        "            bucket_close=(\"tf_final_close\", \"first\"),\n",
        "            bucket_high=(\"tf_final_high\", \"first\"),\n",
        "            bucket_low=(\"tf_final_low\", \"first\"),\n",
        "            bucket_minutes=(\"minutes_total\", \"first\"),\n",
        "            bucket_target=(\"target_up\", \"first\"),\n",
        "        )\n",
        "        .sort_index()\n",
        "    )\n",
        "    bucket_summary[\"bucket_return\"] = (\n",
        "        (bucket_summary[\"bucket_close\"] - bucket_summary[\"bucket_open\"])\n",
        "        / (bucket_summary[\"bucket_open\"] + 1e-9)\n",
        "    )\n",
        "    bucket_summary[\"bucket_range\"] = (\n",
        "        (bucket_summary[\"bucket_high\"] - bucket_summary[\"bucket_low\"])\n",
        "        / (bucket_summary[\"bucket_open\"] + 1e-9)\n",
        "    )\n",
        "    bucket_summary[\"prev_bucket_return\"] = bucket_summary[\"bucket_return\"].shift(1)\n",
        "    bucket_summary[\"prev_bucket_target\"] = bucket_summary[\"bucket_target\"].shift(1)\n",
        "    bucket_summary[\"prev_bucket_range\"] = bucket_summary[\"bucket_range\"].shift(1)\n",
        "\n",
        "    local = local.join(\n",
        "        bucket_summary[\n",
        "            [\n",
        "                \"prev_bucket_return\",\n",
        "                \"prev_bucket_target\",\n",
        "                \"prev_bucket_range\",\n",
        "            ]\n",
        "        ],\n",
        "        on=\"bucket_key\",\n",
        "    )\n",
        "\n",
        "    local[\"prev_bucket_return\"].fillna(0.0, inplace=True)\n",
        "    local[\"prev_bucket_target\"].fillna(0.5, inplace=True)\n",
        "    local[\"prev_bucket_range\"].fillna(0.0, inplace=True)\n",
        "\n",
        "    local[\"timeframe\"] = label\n",
        "    local[\"contract_id\"] = (\n",
        "        label\n",
        "        + \"_\"\n",
        "        + local[\"bucket_start_et\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
        "    )\n",
        "    return local\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7977f695",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_timeframe_dataset(\n",
        "    df: pd.DataFrame,\n",
        "    mapping: Dict[str, str],\n",
        "    target_tz: str | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Assemble les flux enrichis pour chaque horizon demand√©.\"\"\"\n",
        "    tz = target_tz\n",
        "    if tz is None:\n",
        "        try:\n",
        "            tz = TARGET_TZ\n",
        "        except NameError:\n",
        "            tz = \"America/New_York\"\n",
        "\n",
        "    frames = []\n",
        "    for label, freq in mapping.items():\n",
        "        frame = build_timeframe_snapshots(df, freq=freq, label=label, target_tz=tz)\n",
        "        frames.append(frame)\n",
        "    combined = pd.concat(frames).sort_index()\n",
        "    combined = combined[combined[\"minutes_remaining\"] > 0]\n",
        "    combined = combined.dropna(subset=[\"tf_open\", \"tf_close_to_now\"])\n",
        "    return combined\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9954ec9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_preopen_dataset(\n",
        "    minute_df: pd.DataFrame,\n",
        "    mapping: Dict[str, str],\n",
        "    target_tz: str | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Construit un dataset avant l'ouverture de chaque bougie cible.\"\"\"\n",
        "    tz = target_tz or TARGET_TZ\n",
        "\n",
        "    records = []\n",
        "    for label, freq in mapping.items():\n",
        "        full_snapshots = build_timeframe_snapshots(\n",
        "            minute_df,\n",
        "            freq=freq,\n",
        "            label=label,\n",
        "            target_tz=tz,\n",
        "        )\n",
        "\n",
        "        bucket_meta = (\n",
        "            full_snapshots.groupby(\"contract_id\")\n",
        "            .agg(\n",
        "                bucket_start_utc=(\"bucket_start_utc\", \"first\"),\n",
        "                bucket_start_et=(\"bucket_start_et\", \"first\"),\n",
        "                target_up=(\"target_up\", \"first\"),\n",
        "                prev_bucket_return=(\"prev_bucket_return\", \"first\"),\n",
        "                prev_bucket_target=(\"prev_bucket_target\", \"first\"),\n",
        "                prev_bucket_range=(\"prev_bucket_range\", \"first\"),\n",
        "            )\n",
        "            .reset_index()\n",
        "            .sort_values(\"bucket_start_utc\")\n",
        "        )\n",
        "\n",
        "        for _, row in bucket_meta.iterrows():\n",
        "            snapshot_time = row[\"bucket_start_utc\"] - pd.Timedelta(minutes=1)\n",
        "            if snapshot_time not in minute_df.index:\n",
        "                continue\n",
        "            base = minute_df.loc[snapshot_time]\n",
        "            minute_et = snapshot_time.tz_convert(tz)\n",
        "            minute_of_day = minute_et.hour * 60 + minute_et.minute\n",
        "            day_of_week = minute_et.dayofweek\n",
        "            records.append(\n",
        "                {\n",
        "                    \"timeframe\": label,\n",
        "                    \"contract_id\": row[\"contract_id\"],\n",
        "                    \"snapshot_utc\": snapshot_time,\n",
        "                    \"bucket_start_utc\": row[\"bucket_start_utc\"],\n",
        "                    \"target_up\": row[\"target_up\"],\n",
        "                    \"prev_bucket_return\": row[\"prev_bucket_return\"],\n",
        "                    \"prev_bucket_target\": row[\"prev_bucket_target\"],\n",
        "                    \"prev_bucket_range\": row[\"prev_bucket_range\"],\n",
        "                    \"ema_12\": base[\"ema_12\"],\n",
        "                    \"ema_48\": base[\"ema_48\"],\n",
        "                    \"ema_288\": base[\"ema_288\"],\n",
        "                    \"trend_ema_ratio\": base[\"trend_ema_ratio\"],\n",
        "                    \"macro_trend_ratio\": base[\"macro_trend_ratio\"],\n",
        "                    \"rsi_14\": base[\"rsi_14\"],\n",
        "                    \"rolling_vol_30\": base[\"rolling_vol_30\"],\n",
        "                    \"volume_per_minute\": base[\"volume_per_minute\"],\n",
        "                    \"volume_z\": base[\"volume_z\"],\n",
        "                    \"minute_of_day\": minute_of_day,\n",
        "                    \"minute_of_day_sin\": np.sin(2 * np.pi * minute_of_day / 1440),\n",
        "                    \"minute_of_day_cos\": np.cos(2 * np.pi * minute_of_day / 1440),\n",
        "                    \"day_of_week\": day_of_week,\n",
        "                    \"day_of_week_sin\": np.sin(2 * np.pi * day_of_week / 7),\n",
        "                    \"day_of_week_cos\": np.cos(2 * np.pi * day_of_week / 7),\n",
        "                }\n",
        "            )\n",
        "    preopen_df = pd.DataFrame.from_records(records)\n",
        "    preopen_df.sort_values(\"bucket_start_utc\", inplace=True)\n",
        "    return preopen_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88130d86",
      "metadata": {},
      "source": [
        "### 0.3 Fonctions de mod√©lisation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c5861a",
      "metadata": {},
      "outputs": [],
      "source": [
        "FEATURE_COLUMNS = [\n",
        "    \"dist_from_open_pct\",\n",
        "    \"high_gap_pct\",\n",
        "    \"low_gap_pct\",\n",
        "    \"running_range_pct\",\n",
        "    \"minute_body_pct\",\n",
        "    \"time_elapsed_ratio\",\n",
        "    \"time_remaining_ratio\",\n",
        "    \"minutes_elapsed\",\n",
        "    \"minutes_remaining\",\n",
        "    \"minutes_total\",\n",
        "    \"seconds_remaining\",\n",
        "    \"minute_of_day\",\n",
        "    \"minute_of_week\",\n",
        "    \"minute_of_day_sin\",\n",
        "    \"minute_of_day_cos\",\n",
        "    \"day_of_week\",\n",
        "    \"day_of_week_sin\",\n",
        "    \"day_of_week_cos\",\n",
        "    \"prev_bucket_return\",\n",
        "    \"prev_bucket_target\",\n",
        "    \"prev_bucket_range\",\n",
        "    \"return_1m\",\n",
        "    \"ema_12\",\n",
        "    \"ema_48\",\n",
        "    \"ema_288\",\n",
        "    \"trend_ema_ratio\",\n",
        "    \"macro_trend_ratio\",\n",
        "    \"rsi_14\",\n",
        "    \"rolling_vol_30\",\n",
        "    \"volume_per_minute\",\n",
        "    \"volume_z\",\n",
        "    \"is_trend_up\",\n",
        "    \"streak_up_minute\",\n",
        "    \"streak_down_minute\",\n",
        "    \"streak_tf_up\",\n",
        "    \"streak_tf_down\",\n",
        "]\n",
        "\n",
        "TARGET_COLUMN = \"target_up\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009a5dd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sanitize_features(df: pd.DataFrame, features: Iterable[str]) -> pd.DataFrame:\n",
        "    \"\"\"Remplit les valeurs manquantes des features en utilisant la m√©diane.\"\"\"\n",
        "    cleaned = df.copy()\n",
        "    for col in features:\n",
        "        if col not in cleaned:\n",
        "            continue\n",
        "        median = cleaned[col].median()\n",
        "        cleaned[col] = cleaned[col].fillna(median if not np.isnan(median) else 0.0)\n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21aca21",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelBundle:\n",
        "    timeframe: str\n",
        "    model: HistGradientBoostingClassifier\n",
        "    calibrator: LogisticRegression\n",
        "    feature_names: List[str]\n",
        "    metrics: Dict[str, float]\n",
        "    feature_importances: np.ndarray | None = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86b7b447",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_timeframe_models(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: List[str],\n",
        "    target_col: str = TARGET_COLUMN,\n",
        ") -> Dict[str, ModelBundle]:\n",
        "    \"\"\"Entra√Æne un mod√®le par horizon et renvoie les bundles calibr√©s.\"\"\"\n",
        "    bundles: Dict[str, ModelBundle] = {}\n",
        "    for timeframe, frame in df.groupby(\"timeframe\"):\n",
        "        frame = frame.sort_index()\n",
        "        frame = sanitize_features(frame, feature_cols)\n",
        "        frame = frame.dropna(subset=[target_col])\n",
        "\n",
        "        n = len(frame)\n",
        "        if n < 1000:\n",
        "            continue\n",
        "\n",
        "        train_end = int(n * 0.6)\n",
        "        calib_end = int(n * 0.8)\n",
        "\n",
        "        train_slice = frame.iloc[:train_end]\n",
        "        calib_slice = frame.iloc[train_end:calib_end]\n",
        "        test_slice = frame.iloc[calib_end:]\n",
        "\n",
        "        X_train = train_slice[feature_cols]\n",
        "        y_train = train_slice[target_col]\n",
        "\n",
        "        X_calib = calib_slice[feature_cols]\n",
        "        y_calib = calib_slice[target_col]\n",
        "\n",
        "        X_test = test_slice[feature_cols]\n",
        "        y_test = test_slice[target_col]\n",
        "\n",
        "        base_model = HistGradientBoostingClassifier(\n",
        "            learning_rate=0.05,\n",
        "            max_iter=400,\n",
        "            max_depth=6,\n",
        "            l2_regularization=0.01,\n",
        "            min_samples_leaf=80,\n",
        "            random_state=RANDOM_SEED,\n",
        "            scoring=\"loss\",\n",
        "            tol=1e-4,\n",
        "        )\n",
        "        base_model.fit(X_train, y_train)\n",
        "\n",
        "        calib_preds = base_model.predict_proba(X_calib)[:, 1]\n",
        "        calib_preds = calib_preds.reshape(-1, 1)\n",
        "        calibrator = LogisticRegression(max_iter=200)\n",
        "        calibrator.fit(calib_preds, y_calib)\n",
        "\n",
        "        test_raw = base_model.predict_proba(X_test)[:, 1]\n",
        "        test_calibrated = calibrator.predict_proba(test_raw.reshape(-1, 1))[:, 1]\n",
        "\n",
        "        metrics = {\n",
        "            \"roc_auc\": roc_auc_score(y_test, test_calibrated),\n",
        "            \"brier\": brier_score_loss(y_test, test_calibrated),\n",
        "            \"accuracy\": accuracy_score(y_test, (test_calibrated >= 0.5).astype(int)),\n",
        "        }\n",
        "\n",
        "        if hasattr(base_model, \"feature_importances_\"):\n",
        "            feature_importances = base_model.feature_importances_\n",
        "        else:\n",
        "            perm = permutation_importance(\n",
        "                base_model,\n",
        "                X_test,\n",
        "                y_test,\n",
        "                n_repeats=5,\n",
        "                random_state=RANDOM_SEED,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "            feature_importances = perm.importances_mean\n",
        "\n",
        "        bundles[timeframe] = ModelBundle(\n",
        "            timeframe=timeframe,\n",
        "            model=base_model,\n",
        "            calibrator=calibrator,\n",
        "            feature_names=feature_cols,\n",
        "            metrics=metrics,\n",
        "            feature_importances=feature_importances,\n",
        "        )\n",
        "    return bundles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b4de63b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def infer_probabilities(\n",
        "    df: pd.DataFrame,\n",
        "    bundles: Dict[str, ModelBundle],\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Applique les mod√®les calibr√©s √† chaque horizon et renvoie un tableau avec probabilit√©s.\"\"\"\n",
        "    results = []\n",
        "    for timeframe, frame in df.groupby(\"timeframe\"):\n",
        "        bundle = bundles.get(timeframe)\n",
        "        if bundle is None:\n",
        "            continue\n",
        "        frame_prepared = sanitize_features(frame, bundle.feature_names)\n",
        "        raw = bundle.model.predict_proba(frame_prepared[bundle.feature_names])[:, 1]\n",
        "        prob = bundle.calibrator.predict_proba(raw.reshape(-1, 1))[:, 1]\n",
        "        enriched = frame_prepared.copy()\n",
        "        enriched[\"prob_up_raw\"] = raw\n",
        "        enriched[\"prob_up\"] = prob\n",
        "        results.append(enriched)\n",
        "    return pd.concat(results).sort_index()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b0b364a",
      "metadata": {},
      "source": [
        "### 0.4 Fonctions de simulation FOMO et backtest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "073c8045",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class FomoScenario:\n",
        "    name: str\n",
        "    fomo_index: float  # 0 = correction lente, 1 = correction instantan√©e\n",
        "    aggressiveness: float  # amplitude du biais\n",
        "    stickiness: float  # inertie de la cote vis-√†-vis du choc\n",
        "    noise: float = 0.0\n",
        "    alpha: float = 4.0  # poids sur la distance normalis√©e\n",
        "    beta: float = 2.0   # poids sur le range normalis√©\n",
        "    gamma: float = 1.0  # renforcement de fin de p√©riode\n",
        "    k_atr: float = 1.0  # facteur ATR pour la normalisation\n",
        "\n",
        "\n",
        "def simulate_fomo_odds(\n",
        "    df: pd.DataFrame,\n",
        "    scenarios: Iterable[FomoScenario],\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"G√©n√®re des cotes simul√©es minute par minute pour chaque sc√©nario de FOMO.\n",
        "    Utilise une distance normalis√©e par ATR 15m et un renforcement de fin de p√©riode.\"\"\"\n",
        "    simulated = df.copy()\n",
        "\n",
        "    def _simulate_group(group: pd.DataFrame, scenario: FomoScenario) -> np.ndarray:\n",
        "        odds = []\n",
        "        prev_odds = None\n",
        "        eps = 1e-6\n",
        "        for row in group.itertuples():\n",
        "            base = getattr(row, \"prob_up\")\n",
        "            time_decay = getattr(row, \"time_remaining_ratio\")\n",
        "            # distances normalis√©es\n",
        "            atr = max(getattr(row, \"atr_15m\", np.nan) * scenario.k_atr, eps)\n",
        "            z_dist = (getattr(row, \"tf_close_to_now\") - getattr(row, \"tf_open\")) / atr\n",
        "            z_range = (getattr(row, \"tf_high_to_now\") - getattr(row, \"tf_low_to_now\")) / atr\n",
        "\n",
        "            # intensit√© de fin de p√©riode\n",
        "            end_boost = (1 - time_decay) ** scenario.gamma\n",
        "            bias = scenario.aggressiveness * np.tanh(scenario.alpha * z_dist + scenario.beta * z_range) * end_boost\n",
        "\n",
        "            target = float(np.clip(base + bias, 1e-4, 1 - 1e-4))\n",
        "\n",
        "            if prev_odds is None:\n",
        "                proposal = target\n",
        "            else:\n",
        "                proposal = (\n",
        "                    scenario.stickiness * prev_odds\n",
        "                    + (1 - scenario.stickiness) * target\n",
        "                )\n",
        "            blended = (\n",
        "                scenario.fomo_index * base + (1 - scenario.fomo_index) * proposal\n",
        "            )\n",
        "            if scenario.noise > 0:\n",
        "                blended += np.random.normal(0, scenario.noise)\n",
        "            blended = float(np.clip(blended, 1e-4, 1 - 1e-4))\n",
        "            odds.append(blended)\n",
        "            prev_odds = blended\n",
        "        return np.array(odds)\n",
        "\n",
        "    for scenario in scenarios:\n",
        "        column = f\"odds_{scenario.name}\"\n",
        "        simulated[column] = np.nan\n",
        "        for contract_id, group in simulated.groupby(\"contract_id\"):\n",
        "            series = pd.Series(_simulate_group(group, scenario), index=group.index)\n",
        "            simulated.loc[group.index, column] = series\n",
        "    return simulated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba943e5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def equity_curve(trades: pd.DataFrame, stake_usd: float = 50.0) -> pd.Series:\n",
        "    \"\"\"Calcule la courbe d'√©quity cumulative.\"\"\"\n",
        "    if trades.empty:\n",
        "        return pd.Series(dtype=float)\n",
        "    ordered = trades.sort_values(\"timestamp\").copy()\n",
        "    ordered[\"pnl_usd\"] = ordered[\"pnl\"] * stake_usd\n",
        "    curve = ordered[\"pnl_usd\"].cumsum()\n",
        "    curve.index = pd.RangeIndex(len(curve))\n",
        "    return curve\n",
        "\n",
        "\n",
        "def max_drawdown(series: pd.Series) -> float:\n",
        "    \"\"\"Calcule le drawdown maximum (valeur n√©gative).\"\"\"\n",
        "    if series.empty:\n",
        "        return 0.0\n",
        "    cummax = series.cummax()\n",
        "    dd = series - cummax\n",
        "    return float(dd.min())\n",
        "\n",
        "\n",
        "def max_consecutive_losses(trades: pd.DataFrame) -> int:\n",
        "    \"\"\"Calcule le nombre maximum de pertes cons√©cutives.\"\"\"\n",
        "    if trades.empty:\n",
        "        return 0\n",
        "    ordered = trades.sort_values(\"timestamp\")\n",
        "    count = 0\n",
        "    best = 0\n",
        "    for v in (ordered[\"pnl\"] <= 0).astype(int).tolist():\n",
        "        if v == 1:\n",
        "            count += 1\n",
        "            best = max(best, count)\n",
        "        else:\n",
        "            count = 0\n",
        "    return best\n",
        "\n",
        "\n",
        "def build_trades_online_stream(\n",
        "    df: pd.DataFrame,\n",
        "    odds_column: str,\n",
        "    prob_column: str = \"prob_up\",\n",
        "    target_column: str = TARGET_COLUMN,\n",
        "    min_edge: float = 0.05,\n",
        "    min_seconds_remaining: int = 0,\n",
        "    spread_abs: float = 0.05,\n",
        "    fee_abs: float = 0.0,\n",
        "    min_z_abs: Optional[float] = None,\n",
        "    allow_multiple: bool = False,\n",
        "    cooldown_minutes: int = 0,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Parcourt chaque contrat dans l'ordre temporel et d√©clenche une entr√©e au premier d√©passement du seuil.\n",
        "    - Pas de max-edge; d√©cision √† chaud par minute\n",
        "    - Un seul trade par contrat par d√©faut (allow_multiple=False)\n",
        "    - Option de filtre par distance normalis√©e (min_z_abs)\n",
        "    \"\"\"\n",
        "    trades: list[dict] = []\n",
        "\n",
        "    for (timeframe, contract_id), g in df.sort_index().groupby([\"timeframe\", \"contract_id\"], sort=False):\n",
        "        in_cooldown_until = None\n",
        "        entered = False\n",
        "        for row in g.itertuples():\n",
        "            sec_rem = float(getattr(row, \"seconds_remaining\"))\n",
        "            if sec_rem < min_seconds_remaining:\n",
        "                continue\n",
        "            if min_z_abs is not None and hasattr(row, \"z_dist_atr15\"):\n",
        "                if abs(getattr(row, \"z_dist_atr15\")) < min_z_abs:\n",
        "                    continue\n",
        "\n",
        "            ts = row.Index\n",
        "            if in_cooldown_until is not None and ts < in_cooldown_until:\n",
        "                continue\n",
        "\n",
        "            p = float(getattr(row, prob_column))\n",
        "            mid = float(getattr(row, odds_column))\n",
        "            edge = p - mid\n",
        "            if abs(edge) < min_edge:\n",
        "                continue\n",
        "\n",
        "            half_spread = spread_abs / 2.0\n",
        "            if edge > 0:  # acheter YES\n",
        "                direction = \"up\"\n",
        "                paid = np.clip(mid + half_spread, 1e-4, 0.999)\n",
        "                outcome = int(getattr(row, target_column))\n",
        "                pnl = outcome - paid - fee_abs\n",
        "                ev = p - (mid + half_spread) - fee_abs\n",
        "                model_prob_used = p\n",
        "            else:  # acheter NO\n",
        "                direction = \"down\"\n",
        "                paid = np.clip((1 - mid) + half_spread, 1e-4, 0.999)\n",
        "                outcome = 1 - int(getattr(row, target_column))\n",
        "                pnl = outcome - paid - fee_abs\n",
        "                ev = (1 - p) - ((1 - mid) + half_spread) - fee_abs\n",
        "                model_prob_used = 1 - p\n",
        "\n",
        "            trades.append(\n",
        "                {\n",
        "                    \"timeframe\": timeframe,\n",
        "                    \"contract_id\": contract_id,\n",
        "                    \"timestamp\": ts,\n",
        "                    \"seconds_remaining\": sec_rem,\n",
        "                    \"edge\": float(edge),\n",
        "                    \"direction\": direction,\n",
        "                    \"price\": paid,\n",
        "                    \"model_prob\": model_prob_used,\n",
        "                    \"expected_value\": ev,\n",
        "                    \"outcome\": outcome,\n",
        "                    \"pnl\": pnl,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if not allow_multiple:\n",
        "                break\n",
        "            if cooldown_minutes > 0:\n",
        "                in_cooldown_until = ts + pd.Timedelta(minutes=cooldown_minutes)\n",
        "\n",
        "    return pd.DataFrame(trades)\n",
        "\n",
        "\n",
        "def summarize_online_by_timeframe(\n",
        "    df: pd.DataFrame,\n",
        "    odds_column: str,\n",
        "    tolerances: list[float],\n",
        "    min_seconds_remaining_by_tf: Optional[Dict[str, int]] = None,\n",
        "    spread_abs: float = 0.05,\n",
        "    fee_abs: float = 0.0,\n",
        "    min_z_abs: Optional[float] = None,\n",
        "    stake_usd: float = 50.0,\n",
        ") -> pd.DataFrame:\n",
        "    if min_seconds_remaining_by_tf is None:\n",
        "        min_seconds_remaining_by_tf = {\"m15\": 0, \"h1\": 0, \"d1\": 0}\n",
        "\n",
        "    rows: list[dict] = []\n",
        "    for tf in [\"m15\", \"h1\", \"d1\"]:\n",
        "        tf_frame = df[df[\"timeframe\"] == tf]\n",
        "        for tol in tolerances:\n",
        "            tr = build_trades_online_stream(\n",
        "                tf_frame,\n",
        "                odds_column=odds_column,\n",
        "                min_edge=tol,\n",
        "                min_seconds_remaining=min_seconds_remaining_by_tf.get(tf, 0),\n",
        "                spread_abs=spread_abs,\n",
        "                fee_abs=fee_abs,\n",
        "                min_z_abs=min_z_abs,\n",
        "                allow_multiple=False,\n",
        "            )\n",
        "            curve = equity_curve(tr, stake_usd=stake_usd)\n",
        "            mdd = max_drawdown(curve)\n",
        "            mcl = max_consecutive_losses(tr)\n",
        "            med_sec = float(tr[\"seconds_remaining\"].median()) if len(tr) else np.nan\n",
        "            mean_sec = float(tr[\"seconds_remaining\"].mean()) if len(tr) else np.nan\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"timeframe\": tf,\n",
        "                    \"tolerance\": tol,\n",
        "                    \"num_trades\": len(tr),\n",
        "                    \"hit_rate\": float((tr[\"pnl\"] > 0).mean()) if len(tr) else 0.0,\n",
        "                    \"ev_trade_usd\": float(tr[\"pnl\"].mean() * stake_usd) if len(tr) else 0.0,\n",
        "                    \"pnl_total_usd\": float((tr[\"pnl\"] * stake_usd).sum()) if len(tr) else 0.0,\n",
        "                    \"max_drawdown_usd\": float(mdd),\n",
        "                    \"max_consec_losses\": int(mcl),\n",
        "                    \"num_up\": int((tr[\"direction\"]=='up').sum()) if len(tr) else 0,\n",
        "                    \"num_down\": int((tr[\"direction\"]=='down').sum()) if len(tr) else 0,\n",
        "                    \"up_ratio\": float((tr[\"direction\"]=='up').mean()) if len(tr) else 0.0,\n",
        "                    \"median_entry_min\": None if np.isnan(med_sec) else round(med_sec / 60.0, 2),\n",
        "                    \"mean_entry_min\": None if np.isnan(mean_sec) else round(mean_sec / 60.0, 2),\n",
        "                }\n",
        "            )\n",
        "    return pd.DataFrame(rows).sort_values([\"timeframe\", \"tolerance\"])\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5e6d51",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_confidence_bands(\n",
        "    df: pd.DataFrame,\n",
        "    thresholds: Iterable[float],\n",
        "    minute_filter: int | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Mesure la pr√©cision obtenue au-del√† de diff√©rents seuils intrabougie.\"\"\"\n",
        "    records = []\n",
        "    for timeframe, frame in df.groupby(\"timeframe\"):\n",
        "        subset = frame\n",
        "        if minute_filter is not None:\n",
        "            subset = subset[subset[\"minutes_elapsed\"] <= minute_filter]\n",
        "        for thresh in thresholds:\n",
        "            selected = subset[subset[\"prob_up\"] >= thresh]\n",
        "            if selected.empty:\n",
        "                continue\n",
        "            hit_rate = selected[TARGET_COLUMN].mean()\n",
        "            avg_prob = selected[\"prob_up\"].mean()\n",
        "            avg_edge = (selected[\"prob_up\"] - 0.5).mean()\n",
        "            records.append(\n",
        "                {\n",
        "                    \"timeframe\": timeframe,\n",
        "                    \"threshold\": thresh,\n",
        "                    \"count\": len(selected),\n",
        "                    \"hit_rate\": hit_rate,\n",
        "                    \"avg_prob\": avg_prob,\n",
        "                    \"avg_edge_vs_50pct\": avg_edge,\n",
        "                }\n",
        "            )\n",
        "    return pd.DataFrame(records).sort_values([\"timeframe\", \"threshold\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39d482c",
      "metadata": {},
      "outputs": [],
      "source": [
        "FOMO_FEATURE_COLUMNS = [\n",
        "    \"prob_up\",\n",
        "    \"prob_up_raw\",\n",
        "    \"dist_from_open_pct\",\n",
        "    \"high_gap_pct\",\n",
        "    \"low_gap_pct\",\n",
        "    \"running_range_pct\",\n",
        "    \"minute_body_pct\",\n",
        "    \"time_elapsed_ratio\",\n",
        "    \"time_remaining_ratio\",\n",
        "    \"minutes_elapsed\",\n",
        "    \"minutes_remaining\",\n",
        "    \"minutes_total\",\n",
        "    \"seconds_remaining\",\n",
        "    \"z_dist_atr15\",\n",
        "    \"z_range_atr15\",\n",
        "    \"return_1m\",\n",
        "    \"trend_ema_ratio\",\n",
        "    \"macro_trend_ratio\",\n",
        "    \"volume_z\",\n",
        "    \"streak_up_minute\",\n",
        "    \"streak_down_minute\",\n",
        "    \"streak_tf_up\",\n",
        "    \"streak_tf_down\",\n",
        "    \"prev_bucket_return\",\n",
        "    \"prev_bucket_range\",\n",
        "    \"odds_fast_revert\",\n",
        "    \"odds_balanced\",\n",
        "    \"odds_slow_sticky\",\n",
        "]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FomoModelBundle:\n",
        "    timeframe: str\n",
        "    target: str\n",
        "    model: HistGradientBoostingRegressor\n",
        "    feature_names: List[str]\n",
        "    metrics: Dict[str, float]\n",
        "\n",
        "\n",
        "def train_fomo_models(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: List[str],\n",
        "    target_col: str,\n",
        "    target_name: str,\n",
        "    min_rows: int = 500,\n",
        ") -> Dict[str, FomoModelBundle]:\n",
        "    \"\"\"Apprend un r√©gresseur FOMO par timeframe pour approcher les cotes march√©.\"\"\"\n",
        "    bundles: Dict[str, FomoModelBundle] = {}\n",
        "    for timeframe, frame in df.groupby(\"timeframe\"):\n",
        "        frame = frame.sort_index()\n",
        "        frame = frame.dropna(subset=[target_col])\n",
        "        if len(frame) < min_rows:\n",
        "            continue\n",
        "\n",
        "        frame_clean = sanitize_features(frame, feature_cols)\n",
        "        X = frame_clean[feature_cols]\n",
        "        y = frame[target_col]\n",
        "\n",
        "        n = len(frame)\n",
        "        train_end = int(n * 0.7)\n",
        "        val_end = int(n * 0.85)\n",
        "\n",
        "        X_train, y_train = X.iloc[:train_end], y.iloc[:train_end]\n",
        "        X_val, y_val = X.iloc[train_end:val_end], y.iloc[train_end:val_end]\n",
        "        X_test, y_test = X.iloc[val_end:], y.iloc[val_end:]\n",
        "\n",
        "        if len(X_test) < 100:\n",
        "            continue\n",
        "\n",
        "        model = HistGradientBoostingRegressor(\n",
        "            learning_rate=0.05,\n",
        "            max_depth=5,\n",
        "            max_iter=400,\n",
        "            l2_regularization=0.01,\n",
        "            min_samples_leaf=60,\n",
        "            random_state=RANDOM_SEED,\n",
        "        )\n",
        "        model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))\n",
        "\n",
        "        test_pred = model.predict(X_test)\n",
        "        rmse = float(np.sqrt(mean_squared_error(y_test, test_pred)))\n",
        "        mae = float(mean_absolute_error(y_test, test_pred))\n",
        "        r2 = float(r2_score(y_test, test_pred))\n",
        "        similarity_3pct = float((np.abs(test_pred - y_test) <= 0.03).mean())\n",
        "        similarity_5pct = float((np.abs(test_pred - y_test) <= 0.05).mean())\n",
        "\n",
        "        bundles[timeframe] = FomoModelBundle(\n",
        "            timeframe=timeframe,\n",
        "            target=target_name,\n",
        "            model=model,\n",
        "            feature_names=list(feature_cols),\n",
        "            metrics={\n",
        "                \"rmse\": rmse,\n",
        "                \"mae\": mae,\n",
        "                \"r2\": r2,\n",
        "                \"similarity_3pct\": similarity_3pct,\n",
        "                \"similarity_5pct\": similarity_5pct,\n",
        "                \"n_test\": len(X_test),\n",
        "            },\n",
        "        )\n",
        "    return bundles\n",
        "\n",
        "\n",
        "def apply_fomo_models(\n",
        "    df: pd.DataFrame,\n",
        "    bundles: Dict[str, FomoModelBundle],\n",
        "    feature_cols: List[str],\n",
        ") -> pd.Series:\n",
        "    \"\"\"G√©n√®re les pr√©dictions FOMO pour chaque timeframe pr√©sent dans df.\"\"\"\n",
        "    preds = pd.Series(np.nan, index=df.index, dtype=float)\n",
        "    for timeframe, bundle in bundles.items():\n",
        "        mask = df[\"timeframe\"] == timeframe\n",
        "        if not mask.any():\n",
        "            continue\n",
        "        frame = sanitize_features(df.loc[mask], feature_cols)\n",
        "        X = frame[feature_cols]\n",
        "        preds.loc[mask] = bundle.model.predict(X)\n",
        "    return preds\n",
        "\n",
        "\n",
        "def summarize_fomo_performance(\n",
        "    df: pd.DataFrame,\n",
        "    prediction_col: str,\n",
        "    target_col: str,\n",
        "    similarity_threshold: float = 0.03,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Calcule les m√©triques d'alignement mod√®le ‚Üî march√© par timeframe.\"\"\"\n",
        "    records: list[dict] = []\n",
        "    subset = df.dropna(subset=[prediction_col, target_col])\n",
        "    for timeframe, frame in subset.groupby(\"timeframe\"):\n",
        "        diff = frame[prediction_col] - frame[target_col]\n",
        "        rmse = float(np.sqrt((diff**2).mean()))\n",
        "        mae = float(diff.abs().mean())\n",
        "        bias = float(diff.mean())\n",
        "        similarity = float((diff.abs() <= similarity_threshold).mean())\n",
        "        records.append(\n",
        "            {\n",
        "                \"timeframe\": timeframe,\n",
        "                \"n\": len(frame),\n",
        "                \"rmse\": rmse,\n",
        "                \"mae\": mae,\n",
        "                \"bias\": bias,\n",
        "                \"similarity_threshold\": similarity_threshold,\n",
        "                \"similarity_ratio\": similarity,\n",
        "            }\n",
        "        )\n",
        "    return pd.DataFrame(records).sort_values(\"timeframe\")\n",
        "\n",
        "\n",
        "def select_fomo_contract_samples(\n",
        "    df: pd.DataFrame,\n",
        "    timeframe: str,\n",
        "    prediction_col: str,\n",
        "    target_col: str,\n",
        "    n: int = 1,\n",
        ") -> List[str]:\n",
        "    \"\"\"S√©lectionne les contrats √† plus forte erreur absolue moyenne pour inspection.\"\"\"\n",
        "    subset = df[(df[\"timeframe\"] == timeframe)].dropna(subset=[prediction_col, target_col])\n",
        "    if subset.empty:\n",
        "        return []\n",
        "    errors = (\n",
        "        subset.groupby(\"contract_id\")\n",
        "        .apply(lambda g: float(np.abs(g[prediction_col] - g[target_col]).mean()))\n",
        "        .sort_values(ascending=False)\n",
        "    )\n",
        "    return errors.head(n).index.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb411edd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def equity_curve_with_capital(\n",
        "    trades: pd.DataFrame,\n",
        "    initial_capital: float = 1000.0,\n",
        "    strategy_type: str = \"risk_pct\",  # \"risk_pct\" ou \"shares_pct\"\n",
        "    pct_value: float = 0.02,\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"Calcule la courbe d'√©quity et le capital apr√®s chaque trade.\"\"\"\n",
        "    if trades.empty:\n",
        "        return pd.Series(dtype=float), pd.Series(dtype=float)\n",
        "\n",
        "    ordered = trades.sort_values(\"timestamp\").copy()\n",
        "    capital = initial_capital\n",
        "    equity_values: list[float] = []\n",
        "    capital_values: list[float] = []\n",
        "\n",
        "    for _, trade in ordered.iterrows():\n",
        "        price = trade[\"price\"]\n",
        "        if strategy_type == \"risk_pct\":\n",
        "            stake_usd = capital * pct_value\n",
        "            shares = stake_usd / max(price, 1e-6)\n",
        "            pnl_usd = trade[\"pnl\"] * shares\n",
        "        elif strategy_type == \"shares_pct\":\n",
        "            shares = initial_capital * pct_value\n",
        "            pnl_usd = trade[\"pnl\"] * shares\n",
        "        else:\n",
        "            raise ValueError(\"strategy_type doit √™tre 'risk_pct' ou 'shares_pct'\")\n",
        "\n",
        "        capital += pnl_usd\n",
        "        equity_values.append(capital)\n",
        "        capital_values.append(capital)\n",
        "\n",
        "    equity_curve = pd.Series(equity_values, index=pd.RangeIndex(len(equity_values)))\n",
        "    capital_series = pd.Series(capital_values, index=pd.RangeIndex(len(capital_values)))\n",
        "    return equity_curve, capital_series\n",
        "\n",
        "\n",
        "def summarize_online_by_timeframe_with_capital(\n",
        "    df: pd.DataFrame,\n",
        "    odds_column: str,\n",
        "    tolerances: list[float],\n",
        "    initial_capital: float = 1000.0,\n",
        "    strategy_type: str = \"risk_pct\",\n",
        "    pct_value: float = 0.02,\n",
        "    min_seconds_remaining_by_tf: Optional[Dict[str, int]] = None,\n",
        "    spread_abs: float = 0.05,\n",
        "    fee_abs: float = 0.0,\n",
        "    min_z_abs: Optional[float] = None,\n",
        ") -> Tuple[pd.DataFrame, Dict[str, pd.Series]]:\n",
        "    \"\"\"R√©sum√© ONLINE int√©grant la gestion de capital dynamique.\"\"\"\n",
        "    if min_seconds_remaining_by_tf is None:\n",
        "        min_seconds_remaining_by_tf = {\"m15\": 0, \"h1\": 0, \"d1\": 0}\n",
        "\n",
        "    rows: list[dict] = []\n",
        "    equity_curves: Dict[str, pd.Series] = {}\n",
        "    total_contracts = df.groupby(\"timeframe\")[\"contract_id\"].nunique()\n",
        "\n",
        "    for tf in [\"m15\", \"h1\", \"d1\"]:\n",
        "        tf_frame = df[df[\"timeframe\"] == tf]\n",
        "        for tol in tolerances:\n",
        "            tr = build_trades_online_stream(\n",
        "                tf_frame,\n",
        "                odds_column=odds_column,\n",
        "                prob_column=\"prob_up\",\n",
        "                min_edge=tol,\n",
        "                min_seconds_remaining=min_seconds_remaining_by_tf.get(tf, 0),\n",
        "                spread_abs=spread_abs,\n",
        "                fee_abs=fee_abs,\n",
        "                min_z_abs=min_z_abs,\n",
        "                allow_multiple=False,\n",
        "            )\n",
        "\n",
        "            curve, capital_series = equity_curve_with_capital(\n",
        "                tr,\n",
        "                initial_capital=initial_capital,\n",
        "                strategy_type=strategy_type,\n",
        "                pct_value=pct_value,\n",
        "            )\n",
        "\n",
        "            mdd = max_drawdown(curve)\n",
        "            mcl = max_consecutive_losses(tr)\n",
        "            med_sec = float(tr[\"seconds_remaining\"].median()) if len(tr) else np.nan\n",
        "            mean_sec = float(tr[\"seconds_remaining\"].mean()) if len(tr) else np.nan\n",
        "            num_contracts = total_contracts.get(tf, 0)\n",
        "            prob_trade = len(tr) / num_contracts if num_contracts else 0.0\n",
        "\n",
        "            if len(tr) > 0 and not capital_series.empty:\n",
        "                final_capital = float(capital_series.iloc[-1])\n",
        "                pnl_total = final_capital - initial_capital\n",
        "                if strategy_type == \"risk_pct\":\n",
        "                    avg_stake = (capital_series.shift(fill_value=initial_capital) * pct_value).mean()\n",
        "                else:\n",
        "                    shares_count = initial_capital * pct_value\n",
        "                    avg_price = tr[\"price\"].mean()\n",
        "                    avg_stake = shares_count * avg_price\n",
        "                ev_trade = pnl_total / len(tr)\n",
        "            else:\n",
        "                final_capital = initial_capital\n",
        "                pnl_total = 0.0\n",
        "                avg_stake = 0.0\n",
        "                ev_trade = 0.0\n",
        "\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"timeframe\": tf,\n",
        "                    \"tolerance\": tol,\n",
        "                    \"num_trades\": len(tr),\n",
        "                    \"hit_rate\": float((tr[\"pnl\"] > 0).mean()) if len(tr) else 0.0,\n",
        "                    \"ev_trade_usd\": ev_trade,\n",
        "                    \"pnl_total_usd\": pnl_total,\n",
        "                    \"final_capital_usd\": final_capital,\n",
        "                    \"avg_stake_usd\": avg_stake,\n",
        "                    \"max_drawdown_usd\": float(mdd),\n",
        "                    \"max_consec_losses\": int(mcl),\n",
        "                    \"num_up\": int((tr[\"direction\"] == \"up\").sum()) if len(tr) else 0,\n",
        "                    \"num_down\": int((tr[\"direction\"] == \"down\").sum()) if len(tr) else 0,\n",
        "                    \"up_ratio\": float((tr[\"direction\"] == \"up\").mean()) if len(tr) else 0.0,\n",
        "                    \"prob_trade\": prob_trade,\n",
        "                    \"median_entry_min\": None if np.isnan(med_sec) else round(med_sec / 60.0, 2),\n",
        "                    \"mean_entry_min\": None if np.isnan(mean_sec) else round(mean_sec / 60.0, 2),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            equity_curves[f\"{tf}_{tol}\"] = curve\n",
        "\n",
        "    summary = pd.DataFrame(rows).sort_values([\"timeframe\", \"tolerance\"])\n",
        "    return summary, equity_curves\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc1586d3",
      "metadata": {},
      "source": [
        "### 0.5 Fonctions de calibration march√© (optionnelles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d29e60",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_market_csv(csv_path: str) -> pd.DataFrame:\n",
        "    dfm = pd.read_csv(csv_path)\n",
        "    # timestamps ISO8601 en UTC\n",
        "    dfm['timestamp'] = pd.to_datetime(dfm['timestamp'], utc=True, errors='coerce')\n",
        "    dfm = dfm.dropna(subset=['timestamp']).sort_values('timestamp')\n",
        "    dfm = dfm.set_index('timestamp').rename_axis('timestamp_utc')\n",
        "    # mid prices par TF\n",
        "    dfm['m15_mid'] = (dfm['m15_buy'] + dfm['m15_sell'])/2\n",
        "    dfm['h1_mid']  = (dfm['h1_buy']  + dfm['h1_sell'])/2\n",
        "    dfm['d1_mid']  = (dfm['daily_buy']+ dfm['daily_sell'])/2\n",
        "    return dfm\n",
        "\n",
        "def _et(ts_utc: pd.Timestamp) -> pd.Timestamp:\n",
        "    return ts_utc.tz_convert(TARGET_TZ)\n",
        "\n",
        "def _daily_bucket_start_12pm_et(ts_et: pd.Timestamp) -> pd.Timestamp:\n",
        "    # si < 12:00 ET, le bucket a commenc√© la veille 12:00; sinon aujourd'hui 12:00\n",
        "    base = ts_et.floor('D') + pd.Timedelta(hours=12)\n",
        "    return base if ts_et >= base else base - pd.Timedelta(days=1)\n",
        "\n",
        "def build_market_long(dfm: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Agr√©gation par minute (UTC) pour aligner avec les snapshots m1\n",
        "    dfm = dfm.copy()\n",
        "    dfm['minute_bucket'] = dfm.index.floor('1min')\n",
        "    dfm_agg = dfm.groupby('minute_bucket').agg({\n",
        "        'm15_mid': 'mean',\n",
        "        'h1_mid': 'mean',\n",
        "        'd1_mid': 'mean',\n",
        "    }).reset_index()\n",
        "    dfm_agg = dfm_agg.set_index('minute_bucket').rename_axis('timestamp_utc')\n",
        "    dfm_agg['timestamp_et'] = dfm_agg.index.tz_convert(TARGET_TZ)\n",
        "    \n",
        "    rows = []\n",
        "    for ts_utc, row in dfm_agg.iterrows():\n",
        "        ts_et = row['timestamp_et']\n",
        "        # m15\n",
        "        b15_start = ts_et.floor('15min'); b15_end = b15_start + pd.Timedelta(minutes=15)\n",
        "        rows.append({'timeframe':'m15','timestamp_utc':ts_utc,'timestamp_et':ts_et,\n",
        "                     'bucket_start_et':b15_start,'bucket_end_et':b15_end,\n",
        "                     'odds_market_mid': row.get('m15_mid', np.nan)})\n",
        "        # h1\n",
        "        h1_start = ts_et.floor('1h'); h1_end = h1_start + pd.Timedelta(hours=1)\n",
        "        rows.append({'timeframe':'h1','timestamp_utc':ts_utc,'timestamp_et':ts_et,\n",
        "                     'bucket_start_et':h1_start,'bucket_end_et':h1_end,\n",
        "                     'odds_market_mid': row.get('h1_mid', np.nan)})\n",
        "        # d1 (12:00‚Üí12:00 ET)\n",
        "        d1_start = _daily_bucket_start_12pm_et(ts_et); d1_end = d1_start + pd.Timedelta(days=1)\n",
        "        rows.append({'timeframe':'d1','timestamp_utc':ts_utc,'timestamp_et':ts_et,\n",
        "                     'bucket_start_et':d1_start,'bucket_end_et':d1_end,\n",
        "                     'odds_market_mid': row.get('d1_mid', np.nan)})\n",
        "    long = pd.DataFrame(rows).dropna(subset=['odds_market_mid'])\n",
        "    long['bucket_start_utc'] = long['bucket_start_et'].dt.tz_convert('UTC')\n",
        "    long['contract_id'] = long['timeframe'] + '_' + long['bucket_start_et'].dt.strftime('%Y-%m-%d %H:%M')\n",
        "    long['seconds_remaining'] = (long['bucket_end_et'] - long['timestamp_et']).dt.total_seconds()\n",
        "    return long.set_index('timestamp_utc').sort_index()\n",
        "\n",
        "def merge_market_with_sim(simulated_df: pd.DataFrame, market_long: pd.DataFrame) -> pd.DataFrame:\n",
        "    left = simulated_df.copy().reset_index().rename(columns={'index': 'timestamp_utc'})\n",
        "    right = market_long.reset_index().rename(columns={'index': 'timestamp_market_utc'})\n",
        "\n",
        "    merged = pd.merge(\n",
        "        left,\n",
        "        right,\n",
        "        on=['timeframe', 'contract_id'],\n",
        "        how='left',\n",
        "        suffixes=('', '_market'),\n",
        "    )\n",
        "\n",
        "    merged = merged.rename(columns={'timestamp_utc': 'timestamp_model_utc'})\n",
        "    merged = merged.sort_values(['timeframe', 'contract_id', 'timestamp_model_utc'])\n",
        "    merged = merged.set_index('timestamp_model_utc')\n",
        "    merged.index.name = 'timestamp_utc'\n",
        "    return merged\n",
        "\n",
        "def rebuild_ohlc_1m_from_seconds(dfm: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Reconstruit les OHLC 1 minute √† partir des donn√©es spot_price √† la seconde.\"\"\"\n",
        "    dfm = dfm.copy()\n",
        "    dfm['minute_bucket'] = dfm.index.floor('1min')\n",
        "    \n",
        "    ohlc_1m = dfm.groupby('minute_bucket').agg({\n",
        "        'spot_price': ['first', 'max', 'min', 'last', 'count']\n",
        "    })\n",
        "    ohlc_1m.columns = ['open', 'high', 'low', 'close', 'count']\n",
        "    ohlc_1m = ohlc_1m.reset_index()\n",
        "    ohlc_1m = ohlc_1m.set_index('minute_bucket').rename_axis('timestamp_utc')\n",
        "    \n",
        "    # Volume = nombre de ticks par minute (approximation)\n",
        "    ohlc_1m['volume'] = ohlc_1m['count']\n",
        "    ohlc_1m = ohlc_1m.drop('count', axis=1)\n",
        "    \n",
        "    return ohlc_1m\n",
        "\n",
        "\n",
        "def build_market_long_with_stats(dfm: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Reconstruit des lignes minute avec statistiques par bougie (m15/h1/d1).\n",
        "\n",
        "    - Chaque minute contient la cote march√© moyenne (`odds_market_mid`).\n",
        "    - Les statistiques min/max/mean/close sont calcul√©es au niveau de la bougie\n",
        "      (15min, 1h, daily 12pm‚Üí12pm) puis r√©pliqu√©es sur chaque minute appartenant √†\n",
        "      cette bougie.\n",
        "    \"\"\"\n",
        "    dfm = dfm.copy().sort_index()\n",
        "    dfm['timestamp_et'] = dfm.index.tz_convert(TARGET_TZ)\n",
        "    dfm['minute_utc'] = dfm.index.floor('1min')\n",
        "\n",
        "    # Moyennes par minute pour chaque timeframe\n",
        "    minute_means = (\n",
        "        dfm.groupby('minute_utc')[['m15_mid', 'h1_mid', 'd1_mid']].mean().rename(\n",
        "            columns={\n",
        "                'm15_mid': 'm15_mid_minute_mean',\n",
        "                'h1_mid': 'h1_mid_minute_mean',\n",
        "                'd1_mid': 'd1_mid_minute_mean',\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "    minute_means.index = minute_means.index.tz_convert('UTC')\n",
        "    minute_means['timestamp_et'] = minute_means.index.tz_convert(TARGET_TZ)\n",
        "\n",
        "    # Pr√©parer DataFrame final\n",
        "    all_rows: list[dict] = []\n",
        "\n",
        "    def process_timeframe(label: str, minute_col: str, freq: str) -> None:\n",
        "        if label == 'd1':\n",
        "            bucket_start_et = minute_means['timestamp_et'].apply(_daily_bucket_start_12pm_et)\n",
        "            bucket_end_et = bucket_start_et + pd.Timedelta(days=1)\n",
        "            bucket_key = dfm['timestamp_et'].apply(_daily_bucket_start_12pm_et)\n",
        "            stats = (\n",
        "                dfm.groupby(bucket_key)[minute_col.replace('_minute_mean', '')]\n",
        "                .agg(['min', 'max', 'mean', 'last'])\n",
        "                .rename(columns={\n",
        "                    'min': 'odds_market_min',\n",
        "                    'max': 'odds_market_max',\n",
        "                    'mean': 'odds_market_mean',\n",
        "                    'last': 'odds_market_close',\n",
        "                })\n",
        "            )\n",
        "            stats.index.name = 'bucket_start_et'\n",
        "        else:\n",
        "            bucket_start_et = minute_means['timestamp_et'].dt.floor(freq)\n",
        "            bucket_end_et = bucket_start_et + (\n",
        "                pd.Timedelta(minutes=15) if label == 'm15' else pd.Timedelta(hours=1)\n",
        "            )\n",
        "            stats = (\n",
        "                dfm.groupby(dfm['timestamp_et'].dt.floor(freq))[minute_col.replace('_minute_mean', '')]\n",
        "                .agg(['min', 'max', 'mean', 'last'])\n",
        "                .rename(columns={\n",
        "                    'min': 'odds_market_min',\n",
        "                    'max': 'odds_market_max',\n",
        "                    'mean': 'odds_market_mean',\n",
        "                    'last': 'odds_market_close',\n",
        "                })\n",
        "            )\n",
        "            stats.index.name = 'bucket_start_et'\n",
        "        \n",
        "        tf_df = pd.DataFrame({\n",
        "            'timestamp_utc': minute_means.index,\n",
        "            'timestamp_et': minute_means['timestamp_et'],\n",
        "            'bucket_start_et': bucket_start_et,\n",
        "            'bucket_end_et': bucket_end_et,\n",
        "            'odds_market_mid': minute_means[minute_col],\n",
        "        })\n",
        "        tf_df = tf_df.join(stats, on='bucket_start_et')\n",
        "        tf_df['bucket_start_utc'] = tf_df['bucket_start_et'].dt.tz_convert('UTC')\n",
        "        tf_df['contract_id'] = (\n",
        "            label + '_' + tf_df['bucket_start_et'].dt.strftime('%Y-%m-%d %H:%M')\n",
        "        )\n",
        "        tf_df['seconds_remaining'] = (\n",
        "            (tf_df['bucket_end_et'] - tf_df['timestamp_et']).dt.total_seconds()\n",
        "        )\n",
        "        tf_df['timeframe'] = label\n",
        "        all_rows.extend(tf_df.to_dict('records'))\n",
        "\n",
        "    process_timeframe('m15', 'm15_mid_minute_mean', '15min')\n",
        "    process_timeframe('h1', 'h1_mid_minute_mean', '1h')\n",
        "    process_timeframe('d1', 'd1_mid_minute_mean', '1D')\n",
        "\n",
        "    market_long = pd.DataFrame(all_rows)\n",
        "    market_long = market_long.dropna(subset=['odds_market_mid'])\n",
        "    market_long = market_long.sort_values(['timeframe', 'timestamp_utc'])\n",
        "    market_long = market_long.set_index('timestamp_utc')\n",
        "    return market_long\n",
        "\n",
        "\n",
        "def scenario_scores_vs_market(df: pd.DataFrame, scenarios: list[str]) -> pd.DataFrame:\n",
        "    records = []\n",
        "    sub = df.dropna(subset=['odds_market_mid'])\n",
        "    if len(sub) == 0:\n",
        "        return pd.DataFrame(columns=['timeframe', 'scenario', 'rmse', 'mae', 'n'])\n",
        "    for tf, g in sub.groupby('timeframe'):\n",
        "        for sc in scenarios:\n",
        "            col = f'odds_{sc}'\n",
        "            if col not in g.columns:\n",
        "                continue\n",
        "            gg = g.dropna(subset=[col])\n",
        "            if len(gg)==0: continue\n",
        "            err = gg[col] - gg['odds_market_mid']\n",
        "            rmse = float(np.sqrt((err**2).mean()))\n",
        "            mae  = float(err.abs().mean())\n",
        "            records.append({'timeframe':tf,'scenario':sc,'rmse':rmse,'mae':mae,'n':len(gg)})\n",
        "    if len(records) == 0:\n",
        "        return pd.DataFrame(columns=['timeframe', 'scenario', 'rmse', 'mae', 'n'])\n",
        "    return pd.DataFrame(records).sort_values(['timeframe','rmse'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90262ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_market_long_with_minmax(dfm: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Retourne les statistiques mid/min/max/mean/close par bougie march√©.\"\"\"\n",
        "    base = build_market_long_with_stats(dfm)\n",
        "    columns_to_keep = [\n",
        "        \"timeframe\",\n",
        "        \"timestamp_et\",\n",
        "        \"bucket_start_et\",\n",
        "        \"bucket_end_et\",\n",
        "        \"bucket_start_utc\",\n",
        "        \"contract_id\",\n",
        "        \"odds_market_mid\",\n",
        "        \"odds_market_min\",\n",
        "        \"odds_market_max\",\n",
        "        \"odds_market_mean\",\n",
        "        \"odds_market_close\",\n",
        "        \"seconds_remaining\",\n",
        "    ]\n",
        "    available = [col for col in columns_to_keep if col in base.columns]\n",
        "    return base[available]\n",
        "\n",
        "\n",
        "def build_trades_with_minmax_opportunities(\n",
        "    df: pd.DataFrame,\n",
        "    prob_column: str = \"prob_up\",\n",
        "    target_column: str = TARGET_COLUMN,\n",
        "    min_edge: float = 0.05,\n",
        "    min_seconds_remaining: int = 0,\n",
        "    spread_abs: float = 0.05,\n",
        "    fee_abs: float = 0.0,\n",
        "    use_min_for_up: bool = True,\n",
        "    use_max_for_down: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Simule des entr√©es en exploitant les min/max intra-minute des cotes march√©.\"\"\"\n",
        "    trades: list[dict] = []\n",
        "\n",
        "    for (timeframe, contract_id), g in df.sort_index().groupby([\"timeframe\", \"contract_id\"], sort=False):\n",
        "        for row in g.itertuples():\n",
        "            sec_rem = float(getattr(row, \"seconds_remaining\"))\n",
        "            if sec_rem < min_seconds_remaining:\n",
        "                continue\n",
        "\n",
        "            p = float(getattr(row, prob_column))\n",
        "            if p > 0.5:\n",
        "                mid = getattr(row, \"odds_market_mid\", np.nan)\n",
        "                if use_min_for_up and hasattr(row, \"odds_market_min\"):\n",
        "                    mid = getattr(row, \"odds_market_min\")\n",
        "                if np.isnan(mid):\n",
        "                    continue\n",
        "                direction = \"up\"\n",
        "                edge = p - mid\n",
        "                if abs(edge) < min_edge:\n",
        "                    continue\n",
        "                paid = np.clip(mid + spread_abs / 2.0, 1e-4, 0.999)\n",
        "                outcome = int(getattr(row, target_column))\n",
        "                pnl = outcome - paid - fee_abs\n",
        "                ev = p - (mid + spread_abs / 2.0) - fee_abs\n",
        "                model_prob_used = p\n",
        "            else:\n",
        "                mid = getattr(row, \"odds_market_mid\", np.nan)\n",
        "                if use_max_for_down and hasattr(row, \"odds_market_max\"):\n",
        "                    mid = 1.0 - getattr(row, \"odds_market_max\")\n",
        "                else:\n",
        "                    mid = 1.0 - mid\n",
        "                if np.isnan(mid):\n",
        "                    continue\n",
        "                direction = \"down\"\n",
        "                edge = (1 - p) - mid\n",
        "                if abs(edge) < min_edge:\n",
        "                    continue\n",
        "                paid = np.clip(mid + spread_abs / 2.0, 1e-4, 0.999)\n",
        "                outcome = 1 - int(getattr(row, target_column))\n",
        "                pnl = outcome - paid - fee_abs\n",
        "                ev = (1 - p) - (mid + spread_abs / 2.0) - fee_abs\n",
        "                model_prob_used = 1 - p\n",
        "\n",
        "            trades.append(\n",
        "                {\n",
        "                    \"timeframe\": timeframe,\n",
        "                    \"contract_id\": contract_id,\n",
        "                    \"timestamp\": row.Index,\n",
        "                    \"seconds_remaining\": sec_rem,\n",
        "                    \"edge\": float(edge),\n",
        "                    \"direction\": direction,\n",
        "                    \"price\": paid,\n",
        "                    \"model_prob\": model_prob_used,\n",
        "                    \"expected_value\": ev,\n",
        "                    \"outcome\": outcome,\n",
        "                    \"pnl\": pnl,\n",
        "                }\n",
        "            )\n",
        "            break\n",
        "\n",
        "    return pd.DataFrame(trades)\n",
        "\n",
        "\n",
        "def prepare_live_market_stream(market_raw: pd.DataFrame, predictions: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Assemble un flux seconde par seconde avec probabilit√©s minute.\"\"\"\n",
        "    seconds_df = market_raw.copy().sort_index()\n",
        "    seconds_df.index = seconds_df.index.tz_convert(\"UTC\")\n",
        "    seconds_df[\"timestamp_et\"] = seconds_df.index.tz_convert(TARGET_TZ)\n",
        "    seconds_df[\"minute_utc\"] = seconds_df.index.floor(\"1min\")\n",
        "\n",
        "    pred_cols = [\"timeframe\", \"prob_up\", \"target_up\", \"contract_id\"]\n",
        "    pred_meta = (\n",
        "        predictions[pred_cols]\n",
        "        .reset_index()\n",
        "        .rename(columns={\"index\": \"minute_utc\"})\n",
        "    )\n",
        "    pred_meta[\"minute_utc\"] = pred_meta[\"minute_utc\"].dt.tz_convert(\"UTC\")\n",
        "\n",
        "    stream_rows: list[pd.DataFrame] = []\n",
        "\n",
        "    def _bucket_info(frame: pd.Series, tf: str) -> tuple[pd.Series, pd.Series]:\n",
        "        if tf == \"m15\":\n",
        "            bucket_start_et = frame.dt.floor(\"15min\")\n",
        "            bucket_end_et = bucket_start_et + pd.Timedelta(minutes=15)\n",
        "        elif tf == \"h1\":\n",
        "            bucket_start_et = frame.dt.floor(\"1h\")\n",
        "            bucket_end_et = bucket_start_et + pd.Timedelta(hours=1)\n",
        "        else:\n",
        "            bucket_start_et = frame.apply(_daily_bucket_start_12pm_et)\n",
        "            bucket_end_et = bucket_start_et + pd.Timedelta(days=1)\n",
        "        return bucket_start_et, bucket_end_et\n",
        "\n",
        "    for timeframe, col_name in [(\"m15\", \"m15_mid\"), (\"h1\", \"h1_mid\"), (\"d1\", \"d1_mid\")]:\n",
        "        if col_name not in seconds_df.columns:\n",
        "            continue\n",
        "        tf_seconds = (\n",
        "            seconds_df[[\"timestamp_et\", \"minute_utc\", col_name]]\n",
        "            .rename(columns={col_name: \"odds_market_mid\"})\n",
        "            .dropna(subset=[\"odds_market_mid\"])\n",
        "            .copy()\n",
        "        )\n",
        "        if tf_seconds.empty:\n",
        "            continue\n",
        "\n",
        "        bucket_start_et, bucket_end_et = _bucket_info(tf_seconds[\"timestamp_et\"], timeframe)\n",
        "        tf_seconds[\"bucket_start_et\"] = bucket_start_et\n",
        "        tf_seconds[\"bucket_end_et\"] = bucket_end_et\n",
        "        tf_seconds[\"contract_id\"] = timeframe + \"_\" + tf_seconds[\"bucket_start_et\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
        "        tf_seconds[\"seconds_remaining\"] = (tf_seconds[\"bucket_end_et\"] - tf_seconds[\"timestamp_et\"]).dt.total_seconds()\n",
        "        tf_seconds[\"timeframe\"] = timeframe\n",
        "\n",
        "        tf_pred = pred_meta[pred_meta[\"timeframe\"] == timeframe]\n",
        "        merge_cols = [\"minute_utc\", \"contract_id\"]\n",
        "        tf_seconds = tf_seconds.merge(\n",
        "            tf_pred[merge_cols + [\"prob_up\", \"target_up\"]],\n",
        "            on=merge_cols,\n",
        "            how=\"inner\",\n",
        "        )\n",
        "        if tf_seconds.empty:\n",
        "            continue\n",
        "\n",
        "        tf_seconds.index = tf_seconds.index.tz_convert(\"UTC\")\n",
        "        stream_rows.append(tf_seconds)\n",
        "\n",
        "    if not stream_rows:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    stream = pd.concat(stream_rows).sort_index()\n",
        "    cols_order = [\n",
        "        \"timeframe\",\n",
        "        \"contract_id\",\n",
        "        \"seconds_remaining\",\n",
        "        \"prob_up\",\n",
        "        \"target_up\",\n",
        "        \"odds_market_mid\",\n",
        "    ]\n",
        "    cols_extra = [c for c in [\"timestamp_et\", \"bucket_start_et\", \"bucket_end_et\"] if c in stream.columns]\n",
        "    stream = stream[cols_order + cols_extra]\n",
        "    stream.rename_axis(\"timestamp_utc\", inplace=True)\n",
        "    return stream\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36274c2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_preopen_models(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: List[str],\n",
        "    target_col: str = TARGET_COLUMN,\n",
        ") -> Dict[str, ModelBundle]:\n",
        "    \"\"\"Entra√Æne un mod√®le uniquement √† partir des informations pr√©-ouverture.\"\"\"\n",
        "    bundles: Dict[str, ModelBundle] = {}\n",
        "    for timeframe, frame in df.groupby(\"timeframe\"):\n",
        "        frame = frame.dropna(subset=feature_cols + [target_col])\n",
        "        if len(frame) < 500:\n",
        "            continue\n",
        "\n",
        "        n = len(frame)\n",
        "        train_end = int(n * 0.7)\n",
        "        calib_end = int(n * 0.85)\n",
        "\n",
        "        train_slice = frame.iloc[:train_end]\n",
        "        calib_slice = frame.iloc[train_end:calib_end]\n",
        "        test_slice = frame.iloc[calib_end:]\n",
        "\n",
        "        X_train = train_slice[feature_cols]\n",
        "        y_train = train_slice[target_col]\n",
        "\n",
        "        X_calib = calib_slice[feature_cols]\n",
        "        y_calib = calib_slice[target_col]\n",
        "\n",
        "        X_test = test_slice[feature_cols]\n",
        "        y_test = test_slice[target_col]\n",
        "\n",
        "        base_model = HistGradientBoostingClassifier(\n",
        "            learning_rate=0.05,\n",
        "            max_iter=350,\n",
        "            max_depth=5,\n",
        "            l2_regularization=0.015,\n",
        "            min_samples_leaf=60,\n",
        "            random_state=RANDOM_SEED,\n",
        "        )\n",
        "        base_model.fit(X_train, y_train)\n",
        "\n",
        "        calib_preds = base_model.predict_proba(X_calib)[:, 1]\n",
        "        calibrator = LogisticRegression(max_iter=200)\n",
        "        calibrator.fit(calib_preds.reshape(-1, 1), y_calib)\n",
        "\n",
        "        test_raw = base_model.predict_proba(X_test)[:, 1]\n",
        "        test_calibrated = calibrator.predict_proba(test_raw.reshape(-1, 1))[:, 1]\n",
        "\n",
        "        metrics = {\n",
        "            \"roc_auc\": roc_auc_score(y_test, test_calibrated),\n",
        "            \"brier\": brier_score_loss(y_test, test_calibrated),\n",
        "            \"accuracy\": accuracy_score(y_test, (test_calibrated >= 0.5).astype(int)),\n",
        "        }\n",
        "\n",
        "        if hasattr(base_model, \"feature_importances_\"):\n",
        "            feature_importances = base_model.feature_importances_\n",
        "        else:\n",
        "            perm = permutation_importance(\n",
        "                base_model,\n",
        "                X_test,\n",
        "                y_test,\n",
        "                n_repeats=5,\n",
        "                random_state=RANDOM_SEED,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "            feature_importances = perm.importances_mean\n",
        "\n",
        "        bundles[timeframe] = ModelBundle(\n",
        "            timeframe=timeframe,\n",
        "            model=base_model,\n",
        "            calibrator=calibrator,\n",
        "            feature_names=feature_cols,\n",
        "            metrics=metrics,\n",
        "            feature_importances=feature_importances,\n",
        "        )\n",
        "    return bundles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64bcbef1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_preopen_thresholds(\n",
        "    df: pd.DataFrame,\n",
        "    thresholds: Iterable[float],\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Calcule la pr√©cision avant ouverture selon diff√©rents seuils.\"\"\"\n",
        "    records = []\n",
        "    for timeframe, frame in df.groupby(\"timeframe\"):\n",
        "        for thresh in thresholds:\n",
        "            selected = frame[frame[\"prob_up\"] >= thresh]\n",
        "            if selected.empty:\n",
        "                continue\n",
        "            hit_rate = selected[TARGET_COLUMN].mean()\n",
        "            records.append(\n",
        "                {\n",
        "                    \"timeframe\": timeframe,\n",
        "                    \"threshold\": thresh,\n",
        "                    \"count\": len(selected),\n",
        "                    \"hit_rate\": hit_rate,\n",
        "                    \"avg_prob\": selected[\"prob_up\"].mean(),\n",
        "                }\n",
        "            )\n",
        "    return pd.DataFrame(records).sort_values([\"timeframe\", \"threshold\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed389c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_feature_importances(bundle: ModelBundle, top_n: int = 10) -> pd.DataFrame:\n",
        "    \"\"\"Retourne les principales features import√©es d'un mod√®le.\"\"\"\n",
        "    if bundle.feature_importances is None:\n",
        "        raise ValueError(\"Importance des features indisponible pour ce mod√®le\")\n",
        "    data = pd.DataFrame(\n",
        "        {\n",
        "            \"feature\": bundle.feature_names,\n",
        "            \"importance\": bundle.feature_importances,\n",
        "        }\n",
        "    )\n",
        "    data = data.sort_values(\"importance\", ascending=False).head(top_n)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa389583",
      "metadata": {},
      "source": [
        "## 1. Chargement et pr√©paration des donn√©es\n",
        "\n",
        "On charge l'historique BTC m1 (UTC), puis on ajoute les indicateurs globaux n√©cessaires aux mod√®les : rendements, EMA multi-√©chelles, RSI, volatilit√© et signaux de volume.\n",
        "\n",
        "- But: obtenir un DataFrame minute enrichi avec indicateurs techniques.\n",
        "- Entr√©es: CSV OHLCV minute.\n",
        "- Sorties: `minute_df` avec colonnes enrichies (EMA, RSI, ATR, volume, etc.).\n",
        "- Lecture: v√©rifier que les indicateurs sont calcul√©s correctement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25f7276e",
      "metadata": {},
      "outputs": [],
      "source": [
        "minute_df = load_minute_data(DATA_PATH)\n",
        "minute_df = add_global_indicators(minute_df)\n",
        "print(f\"[INFO] Donn√©es charg√©es: {len(minute_df)} lignes de {minute_df.index[0]} √† {minute_df.index[-1]}\")\n",
        "minute_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee9db92",
      "metadata": {},
      "source": [
        "## 2. Reconstruction multi-√©chelle\n",
        "\n",
        "On regroupe chaque horizon (m15, h1, daily) en suivant la timezone ET, en calculant pour chaque minute : ouverture de la bougie cible, range parcouru, temps restant et m√©moire de la bougie pr√©c√©dente.\n",
        "\n",
        "- But: cr√©er des snapshots minute-par-minute pour chaque timeframe avec features intrabougie.\n",
        "- Entr√©es: `minute_df` enrichi.\n",
        "- Sorties: `snapshots_df` avec colonnes par timeframe (contract_id, minutes_remaining, dist_from_open_pct, etc.).\n",
        "- Lecture: v√©rifier que chaque timeframe a bien ses buckets et features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e75847a",
      "metadata": {},
      "outputs": [],
      "source": [
        "TIMEFRAME_MAP = {\"m15\": \"15min\", \"h1\": \"1h\", \"d1\": \"1d\"}\n",
        "\n",
        "snapshots_df = prepare_timeframe_dataset(minute_df, TIMEFRAME_MAP)\n",
        "print(f\"[INFO] Snapshots cr√©√©s: {len(snapshots_df)} lignes\")\n",
        "print(f\"[INFO] R√©partition par timeframe:\")\n",
        "print(snapshots_df.groupby(\"timeframe\").size())\n",
        "snapshots_df.sample(5, random_state=RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8957ae35",
      "metadata": {},
      "source": [
        "## 3. Mod√®les intrabougie\n",
        "\n",
        "Entra√Ænement des classifieurs `HistGradientBoosting` sur chaque horizon avec calibration logistique. On observe ensuite les m√©triques globales et la distribution des probabilit√©s.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a988336a",
      "metadata": {},
      "source": [
        "### 3.1 Entra√Ænement et calibration\n",
        "\n",
        "On entra√Æne un mod√®le `HistGradientBoosting` par horizon (m15, h1, d1) sur les features intrabougie, puis on calibre les probabilit√©s via une r√©gression logistique. On examine ensuite les m√©triques principales pour v√©rifier que le mod√®le d√©passe bien le benchmark 50/50.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36b71035",
      "metadata": {},
      "outputs": [],
      "source": [
        "bundles = train_timeframe_models(snapshots_df, FEATURE_COLUMNS)\n",
        "metrics_overview = {tf: bundle.metrics for tf, bundle in bundles.items()}\n",
        "metrics_overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "476f07d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_df = infer_probabilities(snapshots_df, bundles)\n",
        "pred_df.tail()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "751cf15a",
      "metadata": {},
      "source": [
        "### 3.2 Visualisations des probabilit√©s (ex. m15)\n",
        "\n",
        "Distribution des probabilit√©s pr√©vues et courbe de calibration empirique pour v√©rifier la coh√©rence des scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9892331c",
      "metadata": {},
      "outputs": [],
      "source": [
        "viz_df = pred_df[pred_df[\"timeframe\"] == \"m15\"].copy()\n",
        "if len(viz_df) > 20000:\n",
        "    viz_df = viz_df.sample(20000, random_state=RANDOM_SEED)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "sns.histplot(viz_df[\"prob_up\"], bins=30, ax=axes[0], color=\"#1f77b4\")\n",
        "axes[0].set_title(\"Distribution des probabilit√©s (m15)\")\n",
        "axes[0].set_xlabel(\"Probabilit√© mod√®le d'une cl√¥ture up\")\n",
        "axes[0].set_ylabel(\"Nombre de minutes\")\n",
        "\n",
        "bins = np.linspace(0, 1, 11)\n",
        "labels = pd.IntervalIndex.from_breaks(bins, closed=\"right\")\n",
        "viz_df[\"bucket\"] = pd.cut(viz_df[\"prob_up\"], bins=bins, labels=False, include_lowest=True)\n",
        "calibration = (\n",
        "    viz_df.groupby(\"bucket\").agg(\n",
        "        mean_prob=(\"prob_up\", \"mean\"),\n",
        "        hit_rate=(TARGET_COLUMN, \"mean\"),\n",
        "        count=(TARGET_COLUMN, \"size\"),\n",
        "    )\n",
        ")\n",
        "axes[1].plot([0, 1], [0, 1], ls=\"--\", color=\"gray\", label=\"Parfaitement calibr√©\")\n",
        "axes[1].plot(calibration[\"mean_prob\"], calibration[\"hit_rate\"], marker=\"o\", label=\"Empirique\")\n",
        "axes[1].set_title(\"Calibration empirique (m15)\")\n",
        "axes[1].set_xlabel(\"Probabilit√© moyenne par bin\")\n",
        "axes[1].set_ylabel(\"Fr√©quence r√©alis√©e\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "calibration.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d69898dc",
      "metadata": {},
      "source": [
        "### 3.2.5 Analyse du biais de tendance haussi√®re\n",
        "\n",
        "V√©rification si le mod√®le n'est pas biais√© par une p√©riode de r√©f√©rence en tendance haussi√®re sur Bitcoin. On examine :\n",
        "- La distribution des targets (proportion up vs down)\n",
        "- La distribution des probabilit√©s pr√©dites\n",
        "- La performance du mod√®le sur diff√©rentes p√©riodes\n",
        "- Si le mod√®le pr√©dit syst√©matiquement \"up\" plus souvent que \"down\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e83ea1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyse du biais potentiel de tendance haussi√®re\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ANALYSE DU BIAIS DE TENDANCE HAUSSI√àRE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Distribution des targets (r√©els) par timeframe\n",
        "print(\"\\n1. Distribution des TARGETS r√©els (proportion de bougies qui cl√¥turent UP):\")\n",
        "target_dist = (\n",
        "    snapshots_df.groupby(\"timeframe\")[TARGET_COLUMN]\n",
        "    .agg([\"mean\", \"count\", \"sum\"])\n",
        "    .rename(columns={\"mean\": \"proportion_up\", \"count\": \"total\", \"sum\": \"nb_up\"})\n",
        ")\n",
        "target_dist[\"proportion_down\"] = 1 - target_dist[\"proportion_up\"]\n",
        "target_dist[\"nb_down\"] = target_dist[\"total\"] - target_dist[\"nb_up\"]\n",
        "display(target_dist)\n",
        "\n",
        "# 2. Distribution des probabilit√©s pr√©dites par timeframe\n",
        "print(\"\\n2. Distribution des PROBABILIT√âS PR√âDITES par le mod√®le:\")\n",
        "prob_stats = pred_df.groupby(\"timeframe\")[\"prob_up\"].agg([\n",
        "    \"mean\", \"median\", \"std\", \n",
        "    lambda x: (x > 0.5).mean(),  # proportion de pr√©dictions > 0.5\n",
        "    lambda x: (x > 0.6).mean(),  # proportion de pr√©dictions > 0.6\n",
        "    lambda x: (x < 0.4).mean(),  # proportion de pr√©dictions < 0.4\n",
        "]).rename(columns={\n",
        "    \"<lambda_0>\": \"prop_pred_>_0.5\",\n",
        "    \"<lambda_1>\": \"prop_pred_>_0.6\", \n",
        "    \"<lambda_2>\": \"prop_pred_<_0.4\"\n",
        "})\n",
        "display(prob_stats.round(4))\n",
        "\n",
        "# 3. Comparaison target r√©el vs probabilit√© moyenne pr√©dite\n",
        "print(\"\\n3. Comparaison TARGET R√âEL vs PROBABILIT√â MOYENNE PR√âDITE:\")\n",
        "comparison = pd.DataFrame({\n",
        "    \"target_real_prop_up\": target_dist[\"proportion_up\"],\n",
        "    \"prob_pred_mean\": prob_stats[\"mean\"],\n",
        "    \"diff\": prob_stats[\"mean\"] - target_dist[\"proportion_up\"],\n",
        "})\n",
        "comparison[\"bias_pct\"] = (comparison[\"diff\"] / comparison[\"target_real_prop_up\"] * 100).round(2)\n",
        "display(comparison.round(4))\n",
        "\n",
        "# 4. Performance par p√©riode (d√©coupage temporel)\n",
        "print(\"\\n4. Performance du mod√®le sur diff√©rentes p√©riodes (d√©coupage en 4 quartiles):\")\n",
        "pred_df_with_date = pred_df.copy()\n",
        "pred_df_with_date[\"date\"] = pred_df_with_date.index.date\n",
        "pred_df_with_date[\"period_quartile\"] = pd.qcut(\n",
        "    pred_df_with_date.index.astype(int), q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        ")\n",
        "\n",
        "period_perf = []\n",
        "for tf in [\"m15\", \"h1\", \"d1\"]:\n",
        "    tf_data = pred_df_with_date[pred_df_with_date[\"timeframe\"] == tf]\n",
        "    for period in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
        "        period_data = tf_data[tf_data[\"period_quartile\"] == period]\n",
        "        if len(period_data) == 0:\n",
        "            continue\n",
        "        target_prop = period_data[TARGET_COLUMN].mean()\n",
        "        prob_mean = period_data[\"prob_up\"].mean()\n",
        "        accuracy = ((period_data[\"prob_up\"] >= 0.5) == period_data[TARGET_COLUMN]).mean()\n",
        "        period_perf.append({\n",
        "            \"timeframe\": tf,\n",
        "            \"period\": period,\n",
        "            \"target_prop_up\": target_prop,\n",
        "            \"prob_mean\": prob_mean,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"n\": len(period_data),\n",
        "        })\n",
        "\n",
        "period_df = pd.DataFrame(period_perf)\n",
        "if len(period_df) > 0:\n",
        "    display(period_df.pivot_table(\n",
        "        index=\"timeframe\", \n",
        "        columns=\"period\", \n",
        "        values=[\"target_prop_up\", \"prob_mean\", \"accuracy\"],\n",
        "        aggfunc=\"first\"\n",
        "    ).round(4))\n",
        "\n",
        "# 5. Test de calibration conditionnelle : le mod√®le est-il biais√© vers \"up\"?\n",
        "print(\"\\n5. Test de biais conditionnel:\")\n",
        "print(\"   Si le mod√®le √©tait biais√© par la tendance, on s'attendrait √†:\")\n",
        "print(\"   - Une probabilit√© moyenne pr√©dite > proportion r√©elle de 'up'\")\n",
        "print(\"   - Une meilleure performance sur les p√©riodes haussi√®res\")\n",
        "print(\"   - Des pr√©dictions syst√©matiquement > 0.5\")\n",
        "\n",
        "bias_summary = pd.DataFrame({\n",
        "    \"timeframe\": [\"m15\", \"h1\", \"d1\"],\n",
        "    \"target_real_up\": [target_dist.loc[tf, \"proportion_up\"] for tf in [\"m15\", \"h1\", \"d1\"]],\n",
        "    \"prob_pred_mean\": [prob_stats.loc[tf, \"mean\"] for tf in [\"m15\", \"h1\", \"d1\"]],\n",
        "    \"diff_abs\": [abs(prob_stats.loc[tf, \"mean\"] - target_dist.loc[tf, \"proportion_up\"]) \n",
        "                 for tf in [\"m15\", \"h1\", \"d1\"]],\n",
        "})\n",
        "\n",
        "bias_summary[\"bias_direction\"] = bias_summary.apply(\n",
        "    lambda row: \"surestime UP\" if row[\"prob_pred_mean\"] > row[\"target_real_up\"] \n",
        "    else \"sous-estime UP\", axis=1\n",
        ")\n",
        "bias_summary[\"bias_severity\"] = bias_summary[\"diff_abs\"].apply(\n",
        "    lambda x: \"faible\" if x < 0.02 else \"mod√©r√©\" if x < 0.05 else \"fort\"\n",
        ")\n",
        "\n",
        "display(bias_summary.round(4))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CONCLUSION:\")\n",
        "print(\"=\" * 80)\n",
        "for tf in [\"m15\", \"h1\", \"d1\"]:\n",
        "    target_p = target_dist.loc[tf, \"proportion_up\"]\n",
        "    prob_p = prob_stats.loc[tf, \"mean\"]\n",
        "    diff = prob_p - target_p\n",
        "    print(f\"\\n{tf.upper()}:\")\n",
        "    print(f\"  - Proportion r√©elle de bougies UP: {target_p:.1%}\")\n",
        "    print(f\"  - Probabilit√© moyenne pr√©dite: {prob_p:.1%}\")\n",
        "    print(f\"  - √âcart: {diff:+.1%} ({'surestime' if diff > 0 else 'sous-estime'} UP)\")\n",
        "    if abs(diff) < 0.02:\n",
        "        print(f\"  ‚Üí Biais n√©gligeable (< 2%)\")\n",
        "    elif abs(diff) < 0.05:\n",
        "        print(f\"  ‚Üí Biais mod√©r√©, √† surveiller\")\n",
        "    else:\n",
        "        print(f\"  ‚Üí BIAIS FORT D√âTECT√â - Le mod√®le peut √™tre influenc√© par la tendance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "750a1d0a",
      "metadata": {},
      "source": [
        "### 3.3 Analyse des probabilit√©s\n",
        "\n",
        "On quantifie l'edge en filtrant les signaux dont la probabilit√© d√©passe diff√©rents seuils, √† la fois au cours de la bougie et d√®s la premi√®re minute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b184cb4",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = (\n",
        "    snapshots_df.groupby(\"timeframe\")\n",
        "    .agg(\n",
        "        rows=(\"contract_id\", \"size\"),\n",
        "        contracts=(\"contract_id\", pd.Series.nunique),\n",
        "        median_minutes_total=(\"minutes_total\", \"median\"),\n",
        "    )\n",
        ")\n",
        "display(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e3ebf89",
      "metadata": {},
      "source": [
        "## 4. Probabilit√©s pr√©-ouverture\n",
        "\n",
        "On calcule un snapshot juste avant l'ouverture de chaque pari (m15, h1, daily) pour estimer la direction probable avant m√™me la premi√®re minute de trading.\n",
        "\n",
        "**Fonctionnement du mod√®le pr√©dictif :**\n",
        "- **Pr√©-ouverture** : Probabilit√© calcul√©e √† T-1 min avant l'ouverture, bas√©e sur les features macro (EMA, RSI, tendance, bougie pr√©c√©dente).\n",
        "- **Intrabougie** : Probabilit√© recalcul√©e √† chaque minute pendant la bougie, enrichie par les features intrabougie (distance depuis open, temps restant, etc.).\n",
        "- **Entr√©e conditionnelle** : On peut filtrer par `min_seconds_remaining` pour √©viter d'entrer trop t√¥t quand les probas sont insuffisantes. On peut aussi filtrer par probabilit√© minimale.\n",
        "- **Validation** : On compare la pr√©diction avec la vraie cl√¥ture (target_up) pour calculer le winrate.\n",
        "\n",
        "- But: anticiper la direction d√®s T‚àí1 min.\n",
        "- Entr√©es: `preopen_df`, `PREOPEN_FEATURES`.\n",
        "- Sorties: `preopen_bundles`, `preopen_pred`, tableaux d'√©valuation par seuil.\n",
        "- Lecture: comparer hit_rate par seuils et par TF; utile quand les cotes se forment t√¥t.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "075b937d",
      "metadata": {},
      "outputs": [],
      "source": [
        "thresholds = [0.55, 0.6, 0.65, 0.7]\n",
        "all_minutes_stats = evaluate_confidence_bands(pred_df, thresholds)\n",
        "first_minute_stats = evaluate_confidence_bands(pred_df, thresholds, minute_filter=1)\n",
        "\n",
        "display(all_minutes_stats.head(12))\n",
        "display(first_minute_stats.head(12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb404ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "scenarios = [\n",
        "    FomoScenario(name=\"fast_revert\", fomo_index=0.8, aggressiveness=0.12, stickiness=0.25, noise=0.01),\n",
        "    FomoScenario(name=\"balanced\", fomo_index=0.5, aggressiveness=0.18, stickiness=0.55, noise=0.015),\n",
        "    FomoScenario(name=\"slow_sticky\", fomo_index=0.2, aggressiveness=0.25, stickiness=0.8, noise=0.02),\n",
        "]\n",
        "\n",
        "simulated_df = simulate_fomo_odds(pred_df, scenarios)\n",
        "simulated_df[[\"prob_up\"] + [f\"odds_{s.name}\" for s in scenarios]].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed927fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "preopen_df = build_preopen_dataset(minute_df, TIMEFRAME_MAP)\n",
        "preopen_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca5e158",
      "metadata": {},
      "outputs": [],
      "source": [
        "preopen_bundles = train_preopen_models(preopen_df, PREOPEN_FEATURES)\n",
        "{k: v.metrics for k, v in preopen_bundles.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f09a6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "preopen_pred = infer_probabilities(preopen_df, preopen_bundles)\n",
        "preopen_pred.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4214dfe7",
      "metadata": {},
      "outputs": [],
      "source": [
        "preopen_thresholds = evaluate_preopen_thresholds(preopen_pred, thresholds)\n",
        "preopen_thresholds.head(9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aca4494f",
      "metadata": {},
      "source": [
        "### 4.1 Visualisations des probabilit√©s pr√©-ouverture\n",
        "\n",
        "Graphiques du winrate par timeframe et par seuil de probabilit√© pour les pr√©dictions pr√©-ouverture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea72070",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Winrate par timeframe et seuil\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Graphique 1: Winrate par seuil et timeframe\n",
        "for tf in preopen_thresholds['timeframe'].unique():\n",
        "    tf_data = preopen_thresholds[preopen_thresholds['timeframe'] == tf]\n",
        "    axes[0].plot(tf_data['threshold'], tf_data['hit_rate'], marker='o', label=f'{tf}', linewidth=2)\n",
        "axes[0].axhline(0.5, color='gray', linestyle='--', alpha=0.5, label='50% (baseline)')\n",
        "axes[0].axhline(0.6, color='green', linestyle='--', alpha=0.3, label='60% (objectif minimum)')\n",
        "axes[0].set_xlabel('Seuil de probabilit√©')\n",
        "axes[0].set_ylabel('Winrate (hit rate)')\n",
        "axes[0].set_title('Winrate pr√©-ouverture par timeframe et seuil')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Graphique 2: Nombre de trades par seuil et timeframe\n",
        "for tf in preopen_thresholds['timeframe'].unique():\n",
        "    tf_data = preopen_thresholds[preopen_thresholds['timeframe'] == tf]\n",
        "    axes[1].plot(tf_data['threshold'], tf_data['count'], marker='s', label=f'{tf}', linewidth=2)\n",
        "axes[1].set_xlabel('Seuil de probabilit√©')\n",
        "axes[1].set_ylabel('Nombre de trades')\n",
        "axes[1].set_title('Nombre de trades pr√©-ouverture par timeframe')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tableau r√©capitulatif par timeframe (meilleur seuil pour chaque TF)\n",
        "best_by_tf = preopen_thresholds.loc[preopen_thresholds.groupby('timeframe')['hit_rate'].idxmax()]\n",
        "print(\"Meilleur seuil par timeframe (max winrate):\")\n",
        "display(best_by_tf[['timeframe', 'threshold', 'hit_rate', 'count', 'avg_prob']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb38026a",
      "metadata": {},
      "source": [
        "## 5. Simulation des cotes FOMO\n",
        "\n",
        "On g√©n√®re plusieurs sc√©narios param√©trables (correction rapide, √©quilibr√©e, collante) pour simuler les cotes qu'un march√© d√©s√©quilibr√© pourrait afficher. Ces cotes simul√©es seront ensuite compar√©es aux vraies cotes march√© pour calibrer le mod√®le.\n",
        "\n",
        "- But: cr√©er des cotes synth√©tiques (odds_*) pour tester l'√©cart mod√®le‚Üîmarch√©.\n",
        "- Entr√©es: `pred_df` (probas intrabougie), param√®tres `FomoScenario`.\n",
        "- Sorties: `simulated_df` avec colonnes `odds_<scenario>`.\n",
        "- Lecture: plus (z_dist_atr15, z_range_atr15) et moins de temps restant ‚áí cote plus extr√™me; sinon proche de `prob_up`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb7c533",
      "metadata": {},
      "outputs": [],
      "source": [
        "scenarios = [\n",
        "    FomoScenario(name=\"fast_revert\", fomo_index=0.8, aggressiveness=0.12, stickiness=0.25, noise=0.01),\n",
        "    FomoScenario(name=\"balanced\", fomo_index=0.5, aggressiveness=0.18, stickiness=0.55, noise=0.015),\n",
        "    FomoScenario(name=\"slow_sticky\", fomo_index=0.2, aggressiveness=0.25, stickiness=0.8, noise=0.02),\n",
        "]\n",
        "\n",
        "simulated_df = simulate_fomo_odds(pred_df, scenarios)\n",
        "print(f\"[INFO] Cotes FOMO simul√©es pour {len(scenarios)} sc√©narios\")\n",
        "print(f\"[INFO] Colonnes cr√©√©es: {[f'odds_{s.name}' for s in scenarios]}\")\n",
        "simulated_df[[\"prob_up\"] + [f\"odds_{s.name}\" for s in scenarios]].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca39a2ad",
      "metadata": {},
      "source": [
        "### 5.1 Calibration du mod√®le FOMO\n",
        "\n",
        "On charge les donn√©es BTC.csv (√† la seconde), reconstruit les OHLC 1 minute et les bougies de paris (m15, h1, daily) avec statistiques des cotations (min/max/mean/close). On compare ensuite les cotes simul√©es aux vraies cotes march√© pour calibrer le mod√®le.\n",
        "\n",
        "- But: valider et calibrer le mod√®le FOMO en comparant les cotes simul√©es aux vraies cotes march√©.\n",
        "- Entr√©es: `simulated_df`, `MARKET_ODDS_PATH` (BTC.csv avec donn√©es √† la seconde: timestamp, spot_price, m15_buy, m15_sell, h1_buy, h1_sell, daily_buy, daily_sell).\n",
        "- Sorties: `ohlc_1m_rebuilt` (OHLC 1 minute reconstruits), `market_long` (bougies avec stats: odds_market_min/max/mean/close), `scores`, `best_scenario`, `merged`.\n",
        "- Lecture: les bougies daily sont de 12pm ET √† 12pm ET. Les statistiques des cotations (min/max/mean/close) sont utiles pour les simulations de trading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd9d5ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Les fonctions load_market_csv, build_market_long, merge_market_with_sim et scenario_scores_vs_market\n",
        "# sont d√©j√† d√©finies dans la section 0.5 (Fonctions de calibration march√©).\n",
        "# On v√©rifie si le fichier existe et on charge les donn√©es r√©elles.\n",
        "\n",
        "fomo_models_by_target: Dict[str, Dict[str, FomoModelBundle]] = {}\n",
        "fomo_performance: Dict[str, pd.DataFrame] = {}\n",
        "fomo_target_columns = {\"mean\": \"odds_market_mean\", \"close\": \"odds_market_close\"}\n",
        "fomo_similarity_threshold = 0.03\n",
        "\n",
        "if MARKET_ODDS_PATH.exists():\n",
        "    print(f\"[INFO] Fichier march√© trouv√©: {MARKET_ODDS_PATH}\")\n",
        "    market_raw = load_market_csv(str(MARKET_ODDS_PATH))\n",
        "    print(f\"[INFO] Donn√©es march√© charg√©es: {len(market_raw)} lignes (√† la seconde) de {market_raw.index[0]} √† {market_raw.index[-1]}\")\n",
        "    print(f\"[INFO] Colonnes disponibles: {list(market_raw.columns)}\")\n",
        "    \n",
        "    # Reconstruire les OHLC 1 minute √† partir de spot_price\n",
        "    ohlc_1m_rebuilt = rebuild_ohlc_1m_from_seconds(market_raw)\n",
        "    print(f\"[INFO] OHLC 1 minute reconstruits: {len(ohlc_1m_rebuilt)} bougies\")\n",
        "    print(f\"[INFO] P√©riode OHLC: {ohlc_1m_rebuilt.index[0]} √† {ohlc_1m_rebuilt.index[-1]}\")\n",
        "    \n",
        "    # Reconstruire les bougies de paris avec statistiques des cotations\n",
        "    market_long = build_market_long_with_stats(market_raw)\n",
        "    print(f\"[INFO] Bougies de paris avec stats cr√©√©es: {len(market_long)} lignes\")\n",
        "    print(f\"[INFO] Colonnes disponibles: {list(market_long.columns)}\")\n",
        "    if len(market_long) > 0:\n",
        "        print(f\"[INFO] Exemple de statistiques m15:\")\n",
        "        m15_sample = market_long[market_long['timeframe'] == 'm15'].head(3)\n",
        "        display(m15_sample[['timeframe', 'contract_id', 'odds_market_min', 'odds_market_max', 'odds_market_mean', 'odds_market_close']])\n",
        "    \n",
        "    merged = merge_market_with_sim(simulated_df, market_long)\n",
        "    print(f\"[INFO] Merge simul√©/march√©: {len(merged)} lignes\")\n",
        "    print(f\"[INFO] Lignes avec cotes march√©: {merged['odds_market_mid'].notna().sum()}\")\n",
        "    \n",
        "    # Diagnostics si le merge n'a pas fonctionn√©\n",
        "    if merged['odds_market_mid'].notna().sum() == 0:\n",
        "        print(\"\\n[WARNING] Aucune correspondance trouv√©e entre donn√©es simul√©es et march√©.\")\n",
        "        print(f\"[DEBUG] P√©riode simulated_df: {simulated_df.index.min()} √† {simulated_df.index.max()}\")\n",
        "        print(f\"[DEBUG] P√©riode market_long: {market_long.index.min()} √† {market_long.index.max()}\")\n",
        "        print(f\"[DEBUG] Exemples contract_id simulated_df: {simulated_df['contract_id'].unique()[:5]}\")\n",
        "        print(f\"[DEBUG] Exemples contract_id market_long: {market_long['contract_id'].unique()[:5]}\")\n",
        "        print(f\"[DEBUG] Timeframes simulated_df: {simulated_df['timeframe'].unique()}\")\n",
        "        print(f\"[DEBUG] Timeframes market_long: {market_long['timeframe'].unique()}\")\n",
        "    \n",
        "    # Comparaison des sc√©narios simul√©s vs march√© r√©el\n",
        "    scenario_names = [s.name for s in scenarios]\n",
        "    scores = scenario_scores_vs_market(merged, scenario_names)\n",
        "    print(\"\\n[INFO] Scores RMSE/MAE par sc√©nario et timeframe:\")\n",
        "    display(scores)\n",
        "    \n",
        "    # Meilleur sc√©nario par timeframe (si des scores existent)\n",
        "    if len(scores) > 0:\n",
        "        best_scenario = scores.loc[scores.groupby('timeframe')['rmse'].idxmin()]\n",
        "        print(\"\\n[INFO] Meilleur sc√©nario par timeframe (RMSE minimal):\")\n",
        "        display(best_scenario[['timeframe', 'scenario', 'rmse', 'mae', 'n']])\n",
        "    else:\n",
        "        print(\"\\n[WARNING] Aucun score disponible (pas de correspondances entre simul√© et march√©).\")\n",
        "    \n",
        "    # Entra√Ænement de mod√®les FOMO supervis√©s (cibles moyenne & cl√¥ture)\n",
        "    for target_name, market_col in fomo_target_columns.items():\n",
        "        if market_col not in merged.columns:\n",
        "            continue\n",
        "        available = merged.dropna(subset=[market_col])\n",
        "        if len(available) < 1000:\n",
        "            continue\n",
        "        bundles = train_fomo_models(available, FOMO_FEATURE_COLUMNS, market_col, target_name)\n",
        "        if not bundles:\n",
        "            continue\n",
        "        fomo_models_by_target[target_name] = bundles\n",
        "\n",
        "        merged[f\"fomo_pred_{target_name}\"] = apply_fomo_models(merged, bundles, FOMO_FEATURE_COLUMNS)\n",
        "        simulated_df[f\"fomo_pred_{target_name}\"] = apply_fomo_models(simulated_df, bundles, FOMO_FEATURE_COLUMNS)\n",
        "\n",
        "        metrics_rows = []\n",
        "        for tf, bundle in bundles.items():\n",
        "            row = bundle.metrics.copy()\n",
        "            row[\"timeframe\"] = tf\n",
        "            metrics_rows.append(row)\n",
        "        if metrics_rows:\n",
        "            df_metrics = pd.DataFrame(metrics_rows).sort_values(\"timeframe\")\n",
        "            print(f\"\\n[INFO] M√©triques apprentissage FOMO (cible {target_name})\")\n",
        "            display(df_metrics[[\"timeframe\", \"rmse\", \"mae\", \"r2\", \"similarity_3pct\", \"similarity_5pct\", \"n_test\"]])\n",
        "\n",
        "        perf = summarize_fomo_performance(\n",
        "            merged,\n",
        "            prediction_col=f\"fomo_pred_{target_name}\",\n",
        "            target_col=market_col,\n",
        "            similarity_threshold=fomo_similarity_threshold,\n",
        "        )\n",
        "        if not perf.empty:\n",
        "            print(f\"[INFO] Alignement mod√®le ‚Üî march√© (cible {target_name}, seuil {fomo_similarity_threshold:.2%})\")\n",
        "            display(perf)\n",
        "            fomo_performance[target_name] = perf\n",
        "\n",
        "    # Option: utiliser les cotes march√© r√©elles dans le backtest (si disponibles)\n",
        "    if merged['odds_market_mid'].notna().sum() > 0:\n",
        "        print(\"\\n[INFO] R√©sum√© avec cotes march√© r√©elles:\")\n",
        "        summary_online_market = summarize_online_by_timeframe(\n",
        "            merged, odds_column='odds_market_mid', tolerances=[0.05, 0.10, 0.20, 0.30],\n",
        "            min_seconds_remaining_by_tf={'m15': 0, 'h1': 0, 'd1': 0},\n",
        "            spread_abs=0.05, fee_abs=0.0, min_z_abs=None, stake_usd=50.0\n",
        "        )\n",
        "        display(summary_online_market)\n",
        "    else:\n",
        "        print(\"\\n[WARNING] Impossible de g√©n√©rer le r√©sum√© avec cotes march√© (pas de correspondances).\")\n",
        "    \n",
        "    # On utilise les cotes march√© r√©elles pour la suite seulement si on en a trouv√©\n",
        "    if merged['odds_market_mid'].notna().sum() > 0:\n",
        "        use_market_odds = True\n",
        "        best_odds_column = 'odds_market_mid'\n",
        "    else:\n",
        "        print(\"\\n[INFO] Utilisation du sc√©nario simul√© par d√©faut (slow_sticky) car pas de correspondances march√©.\")\n",
        "        use_market_odds = False\n",
        "        # Si un mod√®le FOMO (moyenne) est disponible, on l'utilise comme meilleure approximation\n",
        "        if 'fomo_pred_mean' in simulated_df.columns:\n",
        "            best_odds_column = 'fomo_pred_mean'\n",
        "        else:\n",
        "            best_odds_column = 'odds_slow_sticky'\n",
        "else:\n",
        "    print(f\"[INFO] Fichier march√© non trouv√©: {MARKET_ODDS_PATH}\")\n",
        "    print(\"[INFO] Utilisation des sc√©narios simul√©s uniquement.\")\n",
        "    use_market_odds = False\n",
        "    # On choisit le sc√©nario par d√©faut (slow_sticky)\n",
        "    best_odds_column = 'odds_slow_sticky'\n",
        "    merged = None\n",
        "    scores = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "415e888c",
      "metadata": {},
      "source": [
        "### 5.2 Visualisation et pr√©cision du mod√®le FOMO\n",
        "\n",
        "Analyse d√©taill√©e de l'alignement entre les cotes pr√©dites par le mod√®le FOMO supervis√© et les cotes march√© r√©elles (moyenne minute et cl√¥ture).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6ad357",
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'merged' not in globals() or merged is None or merged.empty:\n",
        "    print(\"[INFO] Aucun merge simul√©/march√© disponible. Ex√©cuter la section 5.1 avant cette visualisation.\")\n",
        "elif not fomo_models_by_target:\n",
        "    print(\"[INFO] Aucun mod√®le FOMO supervis√© entra√Æn√©. V√©rifiez la disponibilit√© des colonnes march√© et relancez la cellule 5.1.\")\n",
        "else:\n",
        "    summary_tables: list[pd.DataFrame] = []\n",
        "    global_rows: list[dict] = []\n",
        "\n",
        "    for target_name, market_col in fomo_target_columns.items():\n",
        "        pred_col = f\"fomo_pred_{target_name}\"\n",
        "        if pred_col not in merged.columns or market_col not in merged.columns:\n",
        "            continue\n",
        "        perf = summarize_fomo_performance(\n",
        "            merged,\n",
        "            prediction_col=pred_col,\n",
        "            target_col=market_col,\n",
        "            similarity_threshold=fomo_similarity_threshold,\n",
        "        )\n",
        "        if not perf.empty:\n",
        "            perf = perf.copy()\n",
        "            perf[\"target\"] = target_name\n",
        "            perf[\"similarity_%\"] = (perf[\"similarity_ratio\"] * 100).round(2)\n",
        "            summary_tables.append(perf)\n",
        "\n",
        "        subset = merged.dropna(subset=[pred_col, market_col])\n",
        "        if subset.empty:\n",
        "            continue\n",
        "        diff = subset[pred_col] - subset[market_col]\n",
        "        corr = np.corrcoef(subset[market_col], subset[pred_col])[0, 1] if len(subset) > 1 else np.nan\n",
        "        global_rows.append(\n",
        "            {\n",
        "                \"target\": target_name,\n",
        "                \"n_obs\": len(subset),\n",
        "                \"rmse\": float(np.sqrt((diff**2).mean())),\n",
        "                \"mae\": float(diff.abs().mean()),\n",
        "                \"bias\": float(diff.mean()),\n",
        "                \"similarity_3pct\": float((diff.abs() <= 0.03).mean()),\n",
        "                \"similarity_5pct\": float((diff.abs() <= 0.05).mean()),\n",
        "                \"corr\": float(corr),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    if summary_tables:\n",
        "        summary_df = pd.concat(summary_tables, ignore_index=True)\n",
        "        display(summary_df[[\"target\", \"timeframe\", \"n\", \"rmse\", \"mae\", \"bias\", \"similarity_%\"]])\n",
        "    else:\n",
        "        print(\"[INFO] Aucune statistique par timeframe disponible pour les colonnes cibl√©es.\")\n",
        "\n",
        "    if global_rows:\n",
        "        global_df = pd.DataFrame(global_rows)\n",
        "        global_df[[\"similarity_3pct\", \"similarity_5pct\"]] = (global_df[[\"similarity_3pct\", \"similarity_5pct\"]] * 100).round(2)\n",
        "        display(global_df)\n",
        "    else:\n",
        "        print(\"[INFO] Aucun calcul global possible (donn√©es insuffisantes).\")\n",
        "\n",
        "    mean_col = fomo_target_columns.get(\"mean\")\n",
        "    mean_pred_col = \"fomo_pred_mean\"\n",
        "    if mean_col in merged.columns and mean_pred_col in merged.columns:\n",
        "        sample_ids = select_fomo_contract_samples(\n",
        "            merged,\n",
        "            timeframe=\"h1\",\n",
        "            prediction_col=mean_pred_col,\n",
        "            target_col=mean_col,\n",
        "            n=1,\n",
        "        )\n",
        "        if sample_ids:\n",
        "            sample_id = sample_ids[0]\n",
        "            sample = merged[(merged[\"timeframe\"] == \"h1\") & (merged[\"contract_id\"] == sample_id)].sort_index()\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.plot(sample.index, sample[mean_col], label=\"March√© (mean)\")\n",
        "            plt.plot(sample.index, sample[mean_pred_col], label=\"Mod√®le FOMO (mean)\")\n",
        "            if \"odds_balanced\" in sample.columns:\n",
        "                plt.plot(sample.index, sample[\"odds_balanced\"], linestyle=\"--\", alpha=0.6, label=\"Sc√©nario balanced\")\n",
        "            plt.title(f\"Contrat {sample_id} (H1) ‚Äî comparaison cotes moyennes\")\n",
        "            plt.xlabel(\"Timestamp UTC\")\n",
        "            plt.ylabel(\"Probabilit√© de cl√¥ture up\")\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"[INFO] Impossible de s√©lectionner un contrat H1 pour la visualisation (donn√©es insuffisantes).\")\n",
        "    else:\n",
        "        print(\"[INFO] Colonnes n√©cessaires absentes pour la visualisation H1 (mean).\")\n",
        "\n",
        "    close_col = fomo_target_columns.get(\"close\")\n",
        "    close_pred_col = \"fomo_pred_close\"\n",
        "    if close_col in merged.columns and close_pred_col in merged.columns:\n",
        "        sample_ids_close = select_fomo_contract_samples(\n",
        "            merged,\n",
        "            timeframe=\"m15\",\n",
        "            prediction_col=close_pred_col,\n",
        "            target_col=close_col,\n",
        "            n=1,\n",
        "        )\n",
        "        if sample_ids_close:\n",
        "            contract_close = sample_ids_close[0]\n",
        "            sample_close = merged[(merged[\"timeframe\"] == \"m15\") & (merged[\"contract_id\"] == contract_close)].sort_index()\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.plot(sample_close.index, sample_close[close_col], label=\"March√© (close)\")\n",
        "            plt.plot(sample_close.index, sample_close[close_pred_col], label=\"Mod√®le FOMO (close)\")\n",
        "            plt.title(f\"Contrat {contract_close} (M15) ‚Äî comparaison cotes de cl√¥ture\")\n",
        "            plt.xlabel(\"Timestamp UTC\")\n",
        "            plt.ylabel(\"Probabilit√© de cl√¥ture up\")\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"[INFO] Impossible de s√©lectionner un contrat M15 pour la visualisation (close).\")\n",
        "    else:\n",
        "        print(\"[INFO] Colonnes n√©cessaires absentes pour la visualisation M15 (close).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab9029d",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4072ce40",
      "metadata": {},
      "source": [
        "## 6. Backtest ONLINE minute-par-minute\n",
        "\n",
        "D√©cision √† chaud √† chaque minute: on entre √† la premi√®re minute o√π |proba_mod√®le ‚àí cote| ‚â• tol√©rance, sans regard vers le futur, avec spread/frais inclus. Les r√©sultats sont ventil√©s par timeframe (m15/h1/d1).\n",
        "\n",
        "- But: mesurer l'impact r√©el de la tol√©rance (nb trades, EV/trade) en conditions online.\n",
        "- Entr√©es: `simulated_df`, `tolerances`, `spread_abs`, `fee_abs`.\n",
        "- Sorties: `summary_online` (hit_rate, EV/trade, PnL_total, MDD, pertes cons√©cutives, timing).\n",
        "- Lecture: tol√©rance ‚Üë ‚áí nb trades ‚Üì, EV/trade ‚Üë; surveiller MDD et pertes cons√©cutives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38ff2d62",
      "metadata": {},
      "outputs": [],
      "source": [
        "tolerances_online = [0.05, 0.10, 0.20, 0.30]\n",
        "minutes_filters = {\"m15\": 0, \"h1\": 0, \"d1\": 0}\n",
        "\n",
        "if use_market_odds and merged is not None:\n",
        "    backtest_source = merged.dropna(subset=[best_odds_column])\n",
        "    context_label = \"cotes march√©\"\n",
        "else:\n",
        "    backtest_source = simulated_df\n",
        "    context_label = \"cotes simul√©es\"\n",
        "\n",
        "print(f\"[INFO] Backtest ONLINE sur {context_label} avec tol√©rances {tolerances_online}\")\n",
        "summary_online = summarize_online_by_timeframe(\n",
        "    backtest_source,\n",
        "    odds_column=best_odds_column,\n",
        "    tolerances=tolerances_online,\n",
        "    min_seconds_remaining_by_tf=minutes_filters,\n",
        "    spread_abs=0.05,\n",
        "    fee_abs=0.0,\n",
        "    min_z_abs=None,\n",
        "    stake_usd=50.0,\n",
        ")\n",
        "display(summary_online)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473022f8",
      "metadata": {},
      "source": [
        "## 7. Visualisations et analyse\n",
        "\n",
        "On exploite les r√©sultats du backtest minute-par-minute pour diagnostiquer les opportunit√©s : comparaison min/mid/max des cotes march√©, suivi d'√©quity et analyses de biais.\n",
        "\n",
        "- But: explorer graphiquement les performances (√©quity, distributions, timing) et identifier les gains potentiels li√©s aux extr√™mes de cotations.\n",
        "- Entr√©es: `merged_minmax`, `summary_2pct`, `summary_4pct`, `equity_curves_*`.\n",
        "- Sorties: comparatifs statistiques, graphiques d'√©quity, mesures de biais.\n",
        "- Lecture: v√©rifier la robustesse (drawdown, pertes cons√©cutives) et l'impact de l'utilisation min/max sur l'edge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c85f606",
      "metadata": {},
      "outputs": [],
      "source": [
        "if MARKET_ODDS_PATH.exists():\n",
        "    market_raw = load_market_csv(str(MARKET_ODDS_PATH))\n",
        "    market_long_minmax = build_market_long_with_minmax(market_raw)\n",
        "    merged_minmax = merge_market_with_sim(simulated_df, market_long_minmax)\n",
        "\n",
        "    # Comparaison : moyenne vs min/max\n",
        "    trades_mean = build_trades_online_stream(\n",
        "        merged_minmax, odds_column='odds_market_mid', min_edge=0.10,\n",
        "        min_seconds_remaining=0, spread_abs=0.05, fee_abs=0.0\n",
        "    )\n",
        "    trades_minmax = build_trades_with_minmax_opportunities(\n",
        "        merged_minmax, min_edge=0.10,\n",
        "        min_seconds_remaining=0, spread_abs=0.05, fee_abs=0.0,\n",
        "        use_min_for_up=True, use_max_for_down=True\n",
        "    )\n",
        "\n",
        "    print(f\"Trades avec moyenne: {len(trades_mean)}\")\n",
        "    print(f\"Trades avec min/max: {len(trades_minmax)}\")\n",
        "    print(f\"Opportunit√©s suppl√©mentaires: {len(trades_minmax) - len(trades_mean)}\")\n",
        "\n",
        "    # Comparaison PnL\n",
        "    if len(trades_mean) > 0:\n",
        "        print(f\"PnL moyen (moyenne): ${trades_mean['pnl'].mean() * 50:.2f}\")\n",
        "    if len(trades_minmax) > 0:\n",
        "        print(f\"PnL moyen (min/max): ${trades_minmax['pnl'].mean() * 50:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5271d6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# R√©sum√© ONLINE par timeframe avec gestion de capital dynamique\n",
        "odds_col = \"odds_slow_sticky\"   # ou \"odds_balanced\" / \"odds_fast_revert\"\n",
        "tolerances = [0.05, 0.10, 0.20, 0.30]\n",
        "initial_capital = 1000.0\n",
        "\n",
        "# Strat√©gie 1: 2% de risque par pari (montant fixe en USD)\n",
        "print(\"=\" * 80)\n",
        "print(\"STRAT√âGIE 1: 2% de risque par pari (montant fixe en USD)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Exemple: cote 0.2, capital 1000$ ‚Üí stake = 20$, shares = 20/0.2 = 100 shares\")\n",
        "summary_2pct, equity_curves_2pct = summarize_online_by_timeframe_with_capital(\n",
        "    simulated_df,\n",
        "    odds_column=odds_col,\n",
        "    tolerances=tolerances,\n",
        "    initial_capital=initial_capital,\n",
        "    strategy_type=\"risk_pct\",  # Risque X% en USD\n",
        "    pct_value=0.02,  # 2% de risque\n",
        "    min_seconds_remaining_by_tf={\"m15\": 0, \"h1\": 0, \"d1\": 0},\n",
        "    spread_abs=0.05,\n",
        "    fee_abs=0.0,\n",
        "    min_z_abs=None,\n",
        ")\n",
        "display(summary_2pct)\n",
        "\n",
        "# Strat√©gie 2: 4% du capital initial en shares (nombre fixe de shares, co√ªt variable)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STRAT√âGIE 2: 4% du capital initial en shares (nombre fixe, co√ªt variable selon cote)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Exemple: cote 0.2, capital 1000$ ‚Üí shares = 40 (fixe), co√ªt = 40 * 0.2 = 8$\")\n",
        "summary_4pct, equity_curves_4pct = summarize_online_by_timeframe_with_capital(\n",
        "    simulated_df,\n",
        "    odds_column=odds_col,\n",
        "    tolerances=tolerances,\n",
        "    initial_capital=initial_capital,\n",
        "    strategy_type=\"shares_pct\",  # Ach√®te X% du capital en shares\n",
        "    pct_value=0.04,  # 4% du capital en shares\n",
        "    min_seconds_remaining_by_tf={\"m15\": 0, \"h1\": 0, \"d1\": 0},\n",
        "    spread_abs=0.05,\n",
        "    fee_abs=0.0,\n",
        "    min_z_abs=None,\n",
        ")\n",
        "display(summary_4pct)\n",
        "\n",
        "# R√©partition UP/DOWN par timeframe (agr√©g√© toutes tol√©rances) - Strat√©gie 2%\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"R√âPARTITION UP/DOWN (Strat√©gie 2%)\")\n",
        "print(\"=\" * 80)\n",
        "mix_2pct = (summary_2pct\n",
        "       .groupby('timeframe')[['num_up','num_down','num_trades']]\n",
        "       .sum()\n",
        "       .assign(up_ratio=lambda x: (x['num_up'] / x['num_trades']).fillna(0.0)))\n",
        "display(mix_2pct)\n",
        "\n",
        "# Courbes d'√©quity par gestion de risque\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COURBES D'√âQUITY PAR GESTION DE RISQUE\")\n",
        "print(\"=\" * 80)\n",
        "sel_tol = 0.10  # Tol√©rance s√©lectionn√©e pour la visualisation\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for idx, tf in enumerate([\"m15\", \"h1\", \"d1\"]):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Courbe 2%\n",
        "    key_2pct = f\"{tf}_{sel_tol}\"\n",
        "    if key_2pct in equity_curves_2pct and len(equity_curves_2pct[key_2pct]) > 0:\n",
        "        curve_2pct = equity_curves_2pct[key_2pct]\n",
        "        ax.plot(curve_2pct.index, curve_2pct.values, label=f\"2% risque (USD fixe)\", linewidth=2, color=\"#1f77b4\")\n",
        "    \n",
        "    # Courbe 4%\n",
        "    key_4pct = f\"{tf}_{sel_tol}\"\n",
        "    if key_4pct in equity_curves_4pct and len(equity_curves_4pct[key_4pct]) > 0:\n",
        "        curve_4pct = equity_curves_4pct[key_4pct]\n",
        "        ax.plot(curve_4pct.index, curve_4pct.values, label=f\"4% shares (fixe, co√ªt variable)\", linewidth=2, color=\"#ff7f0e\")\n",
        "    \n",
        "    # Ligne de r√©f√©rence (capital initial)\n",
        "    ax.axhline(initial_capital, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Capital initial\")\n",
        "    \n",
        "    ax.set_title(f\"√âquity {tf.upper()} (tol={sel_tol})\")\n",
        "    ax.set_xlabel(\"Trades (#)\")\n",
        "    ax.set_ylabel(\"Capital (USD)\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Utiliser summary_2pct pour les analyses suivantes\n",
        "summary_online = summary_2pct\n",
        "mix = mix_2pct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c71ece",
      "metadata": {},
      "outputs": [],
      "source": [
        "# V√©rification du biais de tendance : comparaison trades vs r√©alit√©\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"V√âRIFICATION OVERFITTING / BIAIS DE TENDANCE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nComparaison entre:\")\n",
        "print(\"  - Proportion de TRADES 'up' (strat√©gie)\")\n",
        "print(\"  - Proportion R√âELLE de bougies qui cl√¥turent 'up' (dataset)\")\n",
        "\n",
        "# Distribution r√©elle des targets par timeframe\n",
        "target_real_dist = (\n",
        "    snapshots_df.groupby(\"timeframe\")[TARGET_COLUMN]\n",
        "    .agg([\"mean\", \"count\"])\n",
        "    .rename(columns={\"mean\": \"target_real_prop_up\", \"count\": \"total_bougies\"})\n",
        ")\n",
        "\n",
        "# Comparaison\n",
        "bias_check = mix[[\"up_ratio\"]].copy()\n",
        "bias_check[\"target_real_prop_up\"] = target_real_dist[\"target_real_prop_up\"]\n",
        "bias_check[\"diff\"] = bias_check[\"up_ratio\"] - bias_check[\"target_real_prop_up\"]\n",
        "bias_check[\"diff_pct\"] = (bias_check[\"diff\"] / bias_check[\"target_real_prop_up\"] * 100).round(2)\n",
        "bias_check[\"bias_severity\"] = bias_check[\"diff\"].abs().apply(\n",
        "    lambda x: \"‚úÖ N√©gligeable\" if x < 0.05 else \"‚ö†Ô∏è Mod√©r√©\" if x < 0.10 else \"‚ùå FORT\"\n",
        ")\n",
        "\n",
        "print(\"\\nR√©sultats par timeframe:\")\n",
        "display(bias_check.round(4))\n",
        "\n",
        "print(\"\\nInterpr√©tation:\")\n",
        "for tf in bias_check.index:\n",
        "    trades_up_pct = bias_check.loc[tf, \"up_ratio\"] * 100\n",
        "    real_up_pct = bias_check.loc[tf, \"target_real_prop_up\"] * 100\n",
        "    diff = bias_check.loc[tf, \"diff\"] * 100\n",
        "    severity = bias_check.loc[tf, \"bias_severity\"]\n",
        "    \n",
        "    print(f\"\\n{tf.upper()}:\")\n",
        "    print(f\"  - Trades 'up': {trades_up_pct:.1f}%\")\n",
        "    print(f\"  - R√©alit√© 'up': {real_up_pct:.1f}%\")\n",
        "    print(f\"  - √âcart: {diff:+.1f}%\")\n",
        "    print(f\"  - {severity}\")\n",
        "    \n",
        "    if abs(diff) > 10:\n",
        "        print(f\"  ‚ö†Ô∏è ATTENTION: La strat√©gie trade {'beaucoup plus' if diff > 0 else 'beaucoup moins'} 'up' que la r√©alit√©.\")\n",
        "        print(f\"     Cela peut indiquer un overfitting sur la tendance haussi√®re du dataset.\")\n",
        "    elif abs(diff) > 5:\n",
        "        print(f\"  ‚ö†Ô∏è √Ä surveiller: l√©ger d√©s√©quilibre d√©tect√©.\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Distribution √©quilibr√©e, pas de biais d√©tect√©.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed61c1cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisations ONLINE\n",
        "# 1) √âquity par timeframe pour une tol√©rance\n",
        "sel_tol = 0.10\n",
        "plt.figure(figsize=(12,6))\n",
        "for tf in [\"m15\",\"h1\",\"d1\"]:\n",
        "    tf_frame = simulated_df[simulated_df[\"timeframe\"] == tf]\n",
        "    tr = build_trades_online_stream(\n",
        "        tf_frame, odds_column=odds_col, min_edge=sel_tol,\n",
        "        min_seconds_remaining=0, spread_abs=0.05, fee_abs=0.0,\n",
        "        min_z_abs=None, allow_multiple=False\n",
        "    )\n",
        "    curve = equity_curve(tr, stake_usd=50.0)\n",
        "    if len(curve) > 0:\n",
        "        plt.plot(curve.index, curve.values, label=f\"{tf}, tol={sel_tol}\")\n",
        "plt.title(f\"√âquity ONLINE (stake 50$, {odds_col})\")\n",
        "plt.xlabel(\"Trades (#)\")\n",
        "plt.ylabel(\"PNL cumul√© (USD)\")\n",
        "plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "# 2) Histogramme PnL par timeframe (m15 en exemple)\n",
        "tf = \"m15\"\n",
        "tr_m15 = build_trades_online_stream(\n",
        "    simulated_df[simulated_df[\"timeframe\"]==tf], odds_column=odds_col, min_edge=sel_tol,\n",
        "    min_seconds_remaining=0, spread_abs=0.05, fee_abs=0.0,\n",
        "    min_z_abs=None, allow_multiple=False\n",
        ")\n",
        "if not tr_m15.empty:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.histplot(tr_m15[\"pnl\"]*50.0, bins=30, color=\"#ff7f0e\")\n",
        "    plt.axvline((tr_m15[\"pnl\"]*50.0).mean(), color=\"black\", ls=\"--\", label=\"PNL moyen\")\n",
        "    plt.title(f\"Distribution PnL USD (ONLINE) ‚Äî {tf}, tol={sel_tol}\")\n",
        "    plt.xlabel(\"PNL par trade (USD)\"); plt.ylabel(\"Nombre de trades\")\n",
        "    plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "# 3) Timing d'entr√©e (minutes avant la cl√¥ture)\n",
        "if not tr_m15.empty:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    mins = tr_m15[\"seconds_remaining\"]/60.0\n",
        "    sns.histplot(mins, bins=30, color=\"#1f77b4\")\n",
        "    plt.title(f\"Timing d'entr√©e ‚Äî {tf}, tol={sel_tol}\")\n",
        "    plt.xlabel(\"Minutes avant la cl√¥ture\"); plt.ylabel(\"Nombre d'entr√©es\")\n",
        "    plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d58d608",
      "metadata": {},
      "source": [
        "## 8. Backtest ONLINE avec cotes march√© r√©elles\n",
        "\n",
        "On rejoue la strat minute-par-minute en utilisant directement les cotes Polymarket (mid prices).\n",
        "Pour chaque timeframe, on simule les entr√©es lorsque le spread mod√®le-march√© d√©passe la tol√©rance.\n",
        "\n",
        "- But: √©valuer la performance realis√©e en branchant les quotes march√©.\n",
        "- Entr√©es: `merged` (probabilit√©s mod√®le + cotes march√©), tol√©rances.\n",
        "- Sorties: tableaux de synth√®se (2% risque / 4% shares) et √©quity comparatives.\n",
        "- Lecture: v√©rifier la coh√©rence des entr√©es (prob_trade), la robustesse (drawdown) et l'impact sizing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b3930c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8.5 Backtest ONLINE avec cotes march√© r√©elles\n",
        "initial_capital_live = 1000.0\n",
        "tolerances_live = [0.05, 0.10, 0.20, 0.30]\n",
        "\n",
        "if 'pred_df' not in globals():\n",
        "    print('[WARN] pred_df absent. Ex√©cutez les sections 3 pour g√©n√©rer les probabilit√©s.')\n",
        "elif not MARKET_ODDS_PATH.exists():\n",
        "    print(f'[WARN] Fichier march√© introuvable: {MARKET_ODDS_PATH}')\n",
        "else:\n",
        "    if 'market_raw' not in globals():\n",
        "        market_raw = load_market_csv(str(MARKET_ODDS_PATH))\n",
        "    live_stream = prepare_live_market_stream(market_raw, pred_df)\n",
        "    if live_stream.empty:\n",
        "        print('[WARN] Impossible de construire le flux live (aucune minute en commun). V√©rifiez les dates des donn√©es mod√®le vs march√©.')\n",
        "    else:\n",
        "        nb_rows = len(live_stream)\n",
        "        nb_contracts = live_stream['contract_id'].nunique()\n",
        "        print(f'[INFO] Flux march√© disponible: {nb_rows} points (secondes), {nb_contracts} contrats.')\n",
        "        print(f'[INFO] Fen√™tre: {live_stream.index.min()} ‚Üí {live_stream.index.max()}')\n",
        "\n",
        "        # Strat√©gie 1: 2% de risque par trade (mise USD fixe)\n",
        "        live_summary_2pct, live_equity_2pct = summarize_online_by_timeframe_with_capital(\n",
        "            live_stream,\n",
        "            odds_column='odds_market_mid',\n",
        "            tolerances=tolerances_live,\n",
        "            initial_capital=initial_capital_live,\n",
        "            strategy_type='risk_pct',\n",
        "            pct_value=0.02,\n",
        "            min_seconds_remaining_by_tf={'m15': 0, 'h1': 0, 'd1': 0},\n",
        "            spread_abs=0.05,\n",
        "            fee_abs=0.0,\n",
        "            min_z_abs=None,\n",
        "        )\n",
        "        print('[INFO] R√©sultats (2% de risque par trade)')\n",
        "        display(live_summary_2pct)\n",
        "\n",
        "        # Strat√©gie 2: 4% du capital initial en shares (nombre fixe de parts)\n",
        "        live_summary_4pct, live_equity_4pct = summarize_online_by_timeframe_with_capital(\n",
        "            live_stream,\n",
        "            odds_column='odds_market_mid',\n",
        "            tolerances=tolerances_live,\n",
        "            initial_capital=initial_capital_live,\n",
        "            strategy_type='shares_pct',\n",
        "            pct_value=0.04,\n",
        "            min_seconds_remaining_by_tf={'m15': 0, 'h1': 0, 'd1': 0},\n",
        "            spread_abs=0.05,\n",
        "            fee_abs=0.0,\n",
        "            min_z_abs=None,\n",
        "        )\n",
        "        print('[INFO] R√©sultats (4% du capital en shares)')\n",
        "        display(live_summary_4pct)\n",
        "\n",
        "        # R√©partition UP/DOWN (strat√©gie 2%)\n",
        "        mix_live = (\n",
        "            live_summary_2pct\n",
        "            .groupby('timeframe')[['num_up', 'num_down', 'num_trades']]\n",
        "            .sum()\n",
        "            .assign(up_ratio=lambda x: (x['num_up'] / x['num_trades']).fillna(0.0))\n",
        "        )\n",
        "        print('[INFO] R√©partition UP/DOWN ‚Äì Strat√©gie 2% de risque')\n",
        "        display(mix_live)\n",
        "\n",
        "        # Visualisation des √©quities pour une tol√©rance donn√©e\n",
        "        sel_tol_live = 0.10\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for tf in ['m15', 'h1', 'd1']:\n",
        "            key = f'{tf}_{sel_tol_live}'\n",
        "            if key in live_equity_2pct and len(live_equity_2pct[key]) > 0:\n",
        "                plt.plot(live_equity_2pct[key].values, label=f'{tf} ‚Äì 2% risque')\n",
        "            if key in live_equity_4pct and len(live_equity_4pct[key]) > 0:\n",
        "                plt.plot(live_equity_4pct[key].values, linestyle='--', label=f'{tf} ‚Äì 4% shares')\n",
        "        plt.axhline(initial_capital_live, color='gray', linestyle='--', alpha=0.4, label='Capital initial')\n",
        "        plt.title(f'√âquit√© r√©elle (tol√©rance = {sel_tol_live})')\n",
        "        plt.xlabel('Trades (#)')\n",
        "        plt.ylabel('Capital (USD)')\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Archive des r√©sum√©s pour exploitation ult√©rieure\n",
        "        live_summary_2pct.to_csv('data/live_backtest_summary_2pct.csv', index=False)\n",
        "        live_summary_4pct.to_csv('data/live_backtest_summary_4pct.csv', index=False)\n",
        "        print('[INFO] R√©sum√©s sauvegard√©s dans data/live_backtest_summary_*.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1a41693",
      "metadata": {},
      "source": [
        "### 8.5 Visualisation des trades (strat√©gie 4¬†% du capital en shares)\n",
        "\n",
        "Cette section synth√©tise les trades issus du backtest r√©el (section¬†8) en se concentrant sur la gestion de taille fixe (4¬†% du capital initial exprim√© en nombre de shares).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e499228b",
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"live_stream\" not in globals() or live_stream.empty:\n",
        "    print(\"[INFO] Aucun flux march√© pr√™t ‚Äî ex√©cuter la section 8 avant cette visualisation.\")\n",
        "elif \"live_summary_4pct\" not in globals():\n",
        "    print(\"[INFO] R√©sultats 4 % shares indisponibles ‚Äî relancer la cellule principale de la section 8.\")\n",
        "else:\n",
        "    share_count = initial_capital_live * 0.04\n",
        "    stats_records: list[dict] = []\n",
        "    trades_frames: list[pd.DataFrame] = []\n",
        "\n",
        "    for tf in [\"m15\", \"h1\", \"d1\"]:\n",
        "        tf_stream = live_stream[live_stream[\"timeframe\"] == tf]\n",
        "        if tf_stream.empty:\n",
        "            continue\n",
        "        for tol in tolerances_live:\n",
        "            trades = build_trades_online_stream(\n",
        "                tf_stream,\n",
        "                odds_column=\"odds_market_mid\",\n",
        "                prob_column=\"prob_up\",\n",
        "                target_column=TARGET_COLUMN,\n",
        "                min_edge=tol,\n",
        "                min_seconds_remaining=0,\n",
        "                spread_abs=0.05,\n",
        "                fee_abs=0.0,\n",
        "                min_z_abs=None,\n",
        "                allow_multiple=False,\n",
        "            )\n",
        "            if trades.empty:\n",
        "                continue\n",
        "\n",
        "            trades = trades.copy()\n",
        "            trades[\"timeframe\"] = tf\n",
        "            trades[\"tolerance\"] = tol\n",
        "            trades[\"pnl_usd\"] = trades[\"pnl\"] * share_count\n",
        "            trades[\"shares\"] = share_count\n",
        "            trades_frames.append(trades)\n",
        "\n",
        "            pnl_usd = trades[\"pnl_usd\"]\n",
        "            pnl_mean = float(pnl_usd.mean())\n",
        "            pnl_std = float(pnl_usd.std(ddof=1))\n",
        "            sharpe = np.nan\n",
        "            if pnl_std > 1e-9:\n",
        "                sharpe = (pnl_mean / pnl_std) * np.sqrt(len(pnl_usd))\n",
        "\n",
        "            stats_records.append(\n",
        "                {\n",
        "                    \"timeframe\": tf,\n",
        "                    \"tolerance\": tol,\n",
        "                    \"num_trades\": len(trades),\n",
        "                    \"winrate\": float((pnl_usd > 0).mean()),\n",
        "                    \"num_up\": int((trades[\"direction\"] == \"up\").sum()),\n",
        "                    \"num_down\": int((trades[\"direction\"] == \"down\").sum()),\n",
        "                    \"avg_pnl_usd\": pnl_mean,\n",
        "                    \"pnl_total_usd\": float(pnl_usd.sum()),\n",
        "                    \"sharpe_trade_sqrtN\": sharpe,\n",
        "                    \"median_edge\": float(trades[\"edge\"].median()),\n",
        "                    \"median_seconds_remaining\": float(trades[\"seconds_remaining\"].median()),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    if not stats_records:\n",
        "        print(\"[INFO] Aucun trade n'a √©t√© g√©n√©r√© pour les param√®tres 4¬†% shares.\")\n",
        "    else:\n",
        "        stats_df = pd.DataFrame(stats_records).sort_values([\"tolerance\", \"timeframe\"])\n",
        "        display(stats_df)\n",
        "\n",
        "        all_trades_df = pd.concat(trades_frames, ignore_index=True).sort_values(\"timestamp\")\n",
        "        pnl_usd = all_trades_df[\"pnl_usd\"]\n",
        "        global_stats = {\n",
        "            \"total_trades\": len(all_trades_df),\n",
        "            \"winrate\": float((pnl_usd > 0).mean()),\n",
        "            \"num_up\": int((all_trades_df[\"direction\"] == \"up\").sum()),\n",
        "            \"num_down\": int((all_trades_df[\"direction\"] == \"down\").sum()),\n",
        "            \"avg_pnl_usd\": float(pnl_usd.mean()),\n",
        "            \"pnl_total_usd\": float(pnl_usd.sum()),\n",
        "            \"median_seconds_remaining\": float(all_trades_df[\"seconds_remaining\"].median()),\n",
        "            \"median_edge\": float(all_trades_df[\"edge\"].median()),\n",
        "        }\n",
        "        pnl_std = float(pnl_usd.std(ddof=1))\n",
        "        if pnl_std > 1e-9:\n",
        "            global_stats[\"sharpe_trade_sqrtN\"] = (global_stats[\"avg_pnl_usd\"] / pnl_std) * np.sqrt(len(pnl_usd))\n",
        "        else:\n",
        "            global_stats[\"sharpe_trade_sqrtN\"] = np.nan\n",
        "\n",
        "        print(\"\\n[INFO] Statistiques agr√©g√©es (toutes tol√©rances / timeframes)\")\n",
        "        display(pd.DataFrame([global_stats]))\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        sns.barplot(data=stats_df, x=\"tolerance\", y=\"pnl_total_usd\", hue=\"timeframe\", ax=axes[0])\n",
        "        axes[0].set_title(\"PnL total par timeframe et tol√©rance\")\n",
        "        axes[0].set_xlabel(\"Tol√©rance\")\n",
        "        axes[0].set_ylabel(\"PnL total (USD)\")\n",
        "        axes[0].legend(title=\"Timeframe\")\n",
        "\n",
        "        sns.barplot(data=stats_df, x=\"tolerance\", y=\"winrate\", hue=\"timeframe\", ax=axes[1])\n",
        "        axes[1].set_title(\"Winrate par timeframe et tol√©rance\")\n",
        "        axes[1].set_xlabel(\"Tol√©rance\")\n",
        "        axes[1].set_ylabel(\"Winrate\")\n",
        "        axes[1].legend(title=\"Timeframe\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a40747",
      "metadata": {},
      "source": [
        "## 9. Lecture mod√®le\n",
        "\n",
        "On inspecte les features dominantes des mod√®les intrabougie et pr√©-ouverture pour comprendre quels signaux le mod√®le exploite le plus.\n",
        "\n",
        "- But: comprendre quels signaux portent l'edge.\n",
        "- Entr√©es: `bundles`, `preopen_bundles`.\n",
        "- Sorties: tableaux d'importances (permutation si besoin).\n",
        "- Lecture: valider l'apport de `z_dist_atr15`, ratios EMA, temps restant, streaks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f23042d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utiliser top_feature_importances d√©fini en section 0 pour l'analyse des mod√®les.\n",
        "\n",
        "for name, bundle in bundles.items():\n",
        "    print(f\"Importance features {name}\")\n",
        "    display(top_feature_importances(bundle))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc1dfc06",
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, bundle in preopen_bundles.items():\n",
        "    print(f\"Importance features pr√©-open {name}\")\n",
        "    display(top_feature_importances(bundle))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6b9012",
      "metadata": {},
      "source": [
        "## 10. Synth√®se et prochaines √©tapes\n",
        "\n",
        "- Les mod√®les intrabougie produisent des probabilit√©s calibr√©es avec des AUC > 0.6 sur l'ensemble test, et d√©passent 70¬†% de hit rate lorsque la probabilit√© mod√®le franchit 0.65 en fin de bougie m15/h1. Les signaux d√®s la premi√®re minute conservent un edge > 60¬†% pour m15 et h1.\n",
        "- Le mod√®le pr√©-ouverture exploite surtout les retours et la structure de tendance (ratios EMA, RSI, volatilit√©). Plusieurs fen√™tres atteignent 60‚Äì62¬†% de r√©ussite sur m15 quand la proba d√©passe 0.6 avant l'ouverture.\n",
        "- La simulation de cotes \"FOMO\" offre trois sc√©narios param√©trables (correction rapide, √©quilibr√©e, collante) ; la strat√©gie value simple garde une PnL moyenne positive (>¬†5¬†% d'EV par trade) dans les configurations √† correction lente.\n",
        "- Tout est pr√™t pour brancher un flux Polymarket r√©el : il suffira d'alimenter `odds_xxx` avec les cotes live, d'ajuster l'indice de FOMO et de monitorer la calibration en temps r√©el.\n",
        "- Prochaines actions : (1) d√©ployer le scrapper temps r√©el, (2) comparer les cotes observ√©es aux sc√©narios simul√©s pour estimer dynamiquement l'indice de FOMO, (3) raffiner la gestion du risque (taille de mise adaptative, limites de liquidit√©) et (4) automatiser l'√©valuation continue via backtests glissants.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
